{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72c49c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cb2dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's analyze the U-Net implementation structure\n",
    "def analyze_unet_structure():\n",
    "    \"\"\"Analyze the U-Net project structure\"\"\"\n",
    "    \n",
    "    print(\"üìÅ U-Net Project Structure:\")\n",
    "    for root, dirs, files in os.walk(\".\"):\n",
    "        level = root.replace('.', '').count(os.sep)\n",
    "        if level > 2:  # Limit depth for readability\n",
    "            continue\n",
    "        indent = ' ' * 2 * level\n",
    "        print(f'{indent}{os.path.basename(root)}/')\n",
    "        subindent = ' ' * 2 * (level + 1)\n",
    "        for file in files:\n",
    "            if file.endswith(('.py', '.md', '.txt', '.ipynb')):\n",
    "                print(f'{subindent}{file}')\n",
    "\n",
    "analyze_unet_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dcf163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the key U-Net implementation files\n",
    "def examine_unet_implementation():\n",
    "    \"\"\"Examine U-Net implementation details\"\"\"\n",
    "    \n",
    "    print(\"üîç U-Net Implementation Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Read the main model file\n",
    "    try:\n",
    "        with open('model.py', 'r') as f:\n",
    "            content = f.read()\n",
    "            print(\"üìÑ model.py key components:\")\n",
    "            \n",
    "            # Look for key U-Net components\n",
    "            components = {\n",
    "                'conv2d': 'Convolutional layers',\n",
    "                'maxpooling2d': 'Downsampling', \n",
    "                'upconv2d': 'Upsampling',\n",
    "                'concatenate': 'Skip connections',\n",
    "                'unet': 'Main model definition'\n",
    "            }\n",
    "            \n",
    "            for key, description in components.items():\n",
    "                if key in content.lower():\n",
    "                    print(f\"   {description} - Found\")\n",
    "                else:\n",
    "                    print(f\"   {description} - Not found\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading model.py: {e}\")\n",
    "    \n",
    "    # Check data preparation\n",
    "    try:\n",
    "        with open('data.py', 'r') as f:\n",
    "            data_content = f.read()\n",
    "            if 'load' in data_content or 'generator' in data_content:\n",
    "                print(\"  Data loading utilities - Found\")\n",
    "    except:\n",
    "        print(\"  ‚ÑπÔ∏è  data.py not found or couldn't be read\")\n",
    "\n",
    "examine_unet_implementation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dee911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's prepare for the TGS Salt Identification Challenge\n",
    "def setup_tgs_salt_dataset():\n",
    "    \"\"\"Setup for TGS Salt Identification Challenge\"\"\"\n",
    "    \n",
    "    print(\"PROBLEM 1: TGS Salt Identification Challenge Setup\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create directory structure for TGS dataset\n",
    "    os.makedirs('tgs_data/train/images', exist_ok=True)\n",
    "    os.makedirs('tgs_data/train/masks', exist_ok=True) \n",
    "    os.makedirs('tgs_data/test/images', exist_ok=True)\n",
    "    os.makedirs('tgs_data/test/masks', exist_ok=True)\n",
    "    \n",
    "    print(\"Created TGS dataset structure:\")\n",
    "    print(\"  - tgs_data/\")\n",
    "    print(\"    - train/\")\n",
    "    print(\"      - images/  # Training seismic images\")\n",
    "    print(\"      - masks/   # Salt segmentation masks\") \n",
    "    print(\"    - test/\")\n",
    "    print(\"      - images/  # Test seismic images\")\n",
    "    print(\"      - masks/   # Test masks (if available)\")\n",
    "    \n",
    "    # Create dataset preparation script\n",
    "    dataset_script = '''\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class TGSSaltDataset:\n",
    "    \"\"\"TGS Salt Identification Challenge Dataset Handler\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path='tgs_data'):\n",
    "        self.data_path = data_path\n",
    "        self.train_csv = os.path.join(data_path, 'train.csv')\n",
    "        \n",
    "    def load_and_prepare_data(self):\n",
    "        \"\"\"Load and prepare TGS Salt dataset\"\"\"\n",
    "        print(\"Loading TGS Salt dataset...\")\n",
    "        \n",
    "        # In a real scenario, you would:\n",
    "        # 1. Download from Kaggle\n",
    "        # 2. Extract zip files\n",
    "        # 3. Organize into train/test splits\n",
    "        \n",
    "        # For demonstration, we'll create a sample structure\n",
    "        self.create_sample_data()\n",
    "        \n",
    "    def create_sample_data(self):\n",
    "        \"\"\"Create sample data structure for demonstration\"\"\"\n",
    "        print(\"üîß Creating sample dataset structure...\")\n",
    "        \n",
    "        # Create sample images and masks\n",
    "        for split in ['train', 'test']:\n",
    "            for i in range(10):  # Create 10 sample images\n",
    "                # Create sample seismic image (128x128 grayscale)\n",
    "                seismic_img = np.random.rand(128, 128) * 255\n",
    "                seismic_img = seismic_img.astype(np.uint8)\n",
    "                \n",
    "                # Create sample salt mask (binary)\n",
    "                salt_mask = np.zeros((128, 128), dtype=np.uint8)\n",
    "                # Add some random salt regions\n",
    "                salt_mask[30:60, 40:80] = 255\n",
    "                \n",
    "                # Save images\n",
    "                img_path = os.path.join(self.data_path, split, 'images', f'sample_{i}.png')\n",
    "                mask_path = os.path.join(self.data_path, split, 'masks', f'sample_{i}.png')\n",
    "                \n",
    "                Image.fromarray(seismic_img).save(img_path)\n",
    "                Image.fromarray(salt_mask).save(mask_path)\n",
    "        \n",
    "        print(f\"Created sample dataset with 10 train and 10 test images\")\n",
    "        \n",
    "    def visualize_sample(self, split='train', idx=0):\n",
    "        \"\"\"Visualize a sample image and mask\"\"\"\n",
    "        img_path = os.path.join(self.data_path, split, 'images', f'sample_{idx}.png')\n",
    "        mask_path = os.path.join(self.data_path, split, 'masks', f'sample_{idx}.png')\n",
    "        \n",
    "        if os.path.exists(img_path) and os.path.exists(mask_path):\n",
    "            img = Image.open(img_path)\n",
    "            mask = Image.open(mask_path)\n",
    "            \n",
    "            fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "            \n",
    "            ax1.imshow(img, cmap='gray')\n",
    "            ax1.set_title('Seismic Image')\n",
    "            ax1.axis('off')\n",
    "            \n",
    "            ax2.imshow(mask, cmap='gray')\n",
    "            ax2.set_title('Salt Mask')\n",
    "            ax2.axis('off')\n",
    "            \n",
    "            # Overlay\n",
    "            ax3.imshow(img, cmap='gray')\n",
    "            ax3.imshow(mask, cmap='Reds', alpha=0.5)\n",
    "            ax3.set_title('Overlay')\n",
    "            ax3.axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"Sample files not found. Run load_and_prepare_data() first.\")\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    dataset = TGSSaltDataset()\n",
    "    dataset.load_and_prepare_data()\n",
    "    dataset.visualize_sample()\n",
    "'''\n",
    "    \n",
    "    with open('tgs_dataset.py', 'w') as f:\n",
    "        f.write(dataset_script)\n",
    "    \n",
    "    print(\"üìú Created TGS dataset handler: tgs_dataset.py\")\n",
    "    \n",
    "    # Create adaptation script for U-Net\n",
    "    adaptation_script = '''\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from model import unet\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "class TGSUnetTrainer:\n",
    "    \"\"\"U-Net trainer adapted for TGS Salt Identification\"\"\"\n",
    "    \n",
    "    def __init__(self, img_rows=128, img_cols=128):\n",
    "        self.img_rows = img_rows\n",
    "        self.img_cols = img_cols\n",
    "        self.model = None\n",
    "        \n",
    "    def load_tgs_data(self):\n",
    "        \"\"\"Load TGS dataset in U-Net compatible format\"\"\"\n",
    "        print(\"Loading TGS data for U-Net...\")\n",
    "        \n",
    "        # This would normally load actual TGS data\n",
    "        # For demo, we create sample data\n",
    "        train_images = []\n",
    "        train_masks = []\n",
    "        \n",
    "        train_dir = 'tgs_data/train/images'\n",
    "        mask_dir = 'tgs_data/train/masks'\n",
    "        \n",
    "        if os.path.exists(train_dir):\n",
    "            for img_file in os.listdir(train_dir)[:10]:  # Limit for demo\n",
    "                if img_file.endswith('.png'):\n",
    "                    # Load image\n",
    "                    img_path = os.path.join(train_dir, img_file)\n",
    "                    img = Image.open(img_path).resize((self.img_rows, self.img_cols))\n",
    "                    img_array = np.array(img) / 255.0\n",
    "                    \n",
    "                    # Load corresponding mask\n",
    "                    mask_path = os.path.join(mask_dir, img_file)\n",
    "                    mask = Image.open(mask_path).resize((self.img_rows, self.img_cols))\n",
    "                    mask_array = np.array(mask) / 255.0\n",
    "                    \n",
    "                    train_images.append(img_array)\n",
    "                    train_masks.append(mask_array)\n",
    "            \n",
    "            train_images = np.array(train_images).reshape(-1, self.img_rows, self.img_cols, 1)\n",
    "            train_masks = np.array(train_masks).reshape(-1, self.img_rows, self.img_cols, 1)\n",
    "            \n",
    "            print(f\" Loaded {len(train_images)} training samples\")\n",
    "            return train_images, train_masks\n",
    "        else:\n",
    "            print(\"Training data not found. Please run tgs_dataset.py first.\")\n",
    "            return None, None\n",
    "    \n",
    "    def create_model(self):\n",
    "        \"\"\"Create U-Net model for salt segmentation\"\"\"\n",
    "        print(\" Creating U-Net model...\")\n",
    "        \n",
    "        # Use the provided U-Net implementation\n",
    "        self.model = unet(input_size=(self.img_rows, self.img_cols, 1))\n",
    "        \n",
    "        # Compile with appropriate loss and metrics for binary segmentation\n",
    "        self.model.compile(optimizer=Adam(lr=1e-4), \n",
    "                          loss='binary_crossentropy',\n",
    "                          metrics=['accuracy'])\n",
    "        \n",
    "        print(\" U-Net model created and compiled\")\n",
    "        return self.model\n",
    "    \n",
    "    def train(self, epochs=10, batch_size=8):\n",
    "        \"\"\"Train the U-Net model\"\"\"\n",
    "        print(\" Starting U-Net training...\")\n",
    "        \n",
    "        # Load data\n",
    "        train_images, train_masks = self.load_tgs_data()\n",
    "        \n",
    "        if train_images is None:\n",
    "            return\n",
    "        \n",
    "        # Create model\n",
    "        self.create_model()\n",
    "        \n",
    "        # Setup callbacks\n",
    "        callbacks = [\n",
    "            ModelCheckpoint('tgs_unet_weights.h5', save_best_only=True),\n",
    "            EarlyStopping(patience=5, restore_best_weights=True)\n",
    "        ]\n",
    "        \n",
    "        # Train model\n",
    "        history = self.model.fit(\n",
    "            train_images, train_masks,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_split=0.2,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(\"Training completed!\")\n",
    "        return history\n",
    "    \n",
    "    def predict(self, image_path):\n",
    "        \"\"\"Make prediction on a single image\"\"\"\n",
    "        if self.model is None:\n",
    "            print(\"Model not trained. Please train first.\")\n",
    "            return None\n",
    "        \n",
    "        # Load and preprocess image\n",
    "        img = Image.open(image_path).convert('L')\n",
    "        img = img.resize((self.img_rows, self.img_cols))\n",
    "        img_array = np.array(img) / 255.0\n",
    "        img_array = img_array.reshape(1, self.img_rows, self.img_cols, 1)\n",
    "        \n",
    "        # Predict\n",
    "        prediction = self.model.predict(img_array)[0]\n",
    "        \n",
    "        # Convert to binary mask\n",
    "        binary_mask = (prediction > 0.5).astype(np.uint8) * 255\n",
    "        \n",
    "        return binary_mask\n",
    "\n",
    "# Training example\n",
    "if __name__ == \"__main__\":\n",
    "    trainer = TGSUnetTrainer()\n",
    "    trainer.train(epochs=5)  # Short training for demo\n",
    "'''\n",
    "    \n",
    "    with open('train_tgs_unet.py', 'w') as f:\n",
    "        f.write(adaptation_script)\n",
    "    \n",
    "    print(\"üîß Created U-Net adaptation for TGS: train_tgs_unet.py\")\n",
    "    \n",
    "    print(\"\\n TGS Salt Identification setup completed!\")\n",
    "    print(\"\\\\nNext steps:\")\n",
    "    print(\"1. Run: python tgs_dataset.py (to create sample data)\")\n",
    "    print(\"2. Run: python train_tgs_unet.py (to train U-Net)\")\n",
    "    print(\"3. Download actual TGS dataset from Kaggle for real training\")\n",
    "\n",
    "setup_tgs_salt_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546a0ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test the dataset creation\n",
    "def test_dataset_creation():\n",
    "    \"\"\"Test the TGS dataset creation\"\"\"\n",
    "    \n",
    "    print(\"üß™ Testing Dataset Creation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Run the dataset creation\n",
    "    !python tgs_dataset.py\n",
    "    \n",
    "    # Visualize samples\n",
    "    try:\n",
    "        from tgs_dataset import TGSSaltDataset\n",
    "        dataset = TGSSaltDataset()\n",
    "        dataset.visualize_sample('train', 0)\n",
    "        dataset.visualize_sample('train', 1)\n",
    "    except Exception as e:\n",
    "        print(f\"Visualization error: {e}\")\n",
    "\n",
    "test_dataset_creation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9135a5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_code_reading():\n",
    "    \"\"\"Code reading for U-Net paper and implementation\"\"\"\n",
    "    \n",
    "    print(\"üéØ PROBLEM 2: U-Net Code Reading\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Key components from U-Net paper\n",
    "    paper_components = {\n",
    "        \"Contracting Path (Encoder)\": \"Feature extraction with downsampling\",\n",
    "        \"Expanding Path (Decoder)\": \"Feature reconstruction with upsampling\", \n",
    "        \"Skip Connections\": \"Connecting encoder and decoder features\",\n",
    "        \"U-Shaped Architecture\": \"Symmetric encoder-decoder structure\",\n",
    "        \"Data Augmentation\": \"Elastic deformations for limited data\",\n",
    "        \"Weighted Loss\": \"Handling class imbalance in biomedical images\",\n",
    "        \"Overlap-tile Strategy\": \"Seamless segmentation of large images\"\n",
    "    }\n",
    "    \n",
    "    print(\"üìã Key Components from U-Net Paper:\")\n",
    "    for component, description in paper_components.items():\n",
    "        print(f\"  ‚Ä¢ {component}: {description}\")\n",
    "    \n",
    "    print(\"\\nüîç Finding corresponding code implementations...\")\n",
    "    \n",
    "    # Read and analyze the model implementation\n",
    "    try:\n",
    "        with open('model.py', 'r') as f:\n",
    "            model_code = f.read()\n",
    "            \n",
    "        print(\"\\nüìÑ U-Net Implementation Analysis:\")\n",
    "        \n",
    "        # Check for key architectural components\n",
    "        key_elements = {\n",
    "            'def unet': 'Main U-Net model definition',\n",
    "            'Conv2D': 'Convolutional layers', \n",
    "            'MaxPooling2D': 'Downsampling in encoder',\n",
    "            'UpSampling2D': 'Upsampling in decoder',\n",
    "            'concatenate': 'Skip connections',\n",
    "            'Dropout': 'Regularization',\n",
    "            'sigmoid': 'Binary segmentation output'\n",
    "        }\n",
    "        \n",
    "        for element, description in key_elements.items():\n",
    "            if element in model_code:\n",
    "                print(f\"  ‚úÖ {description} - Implemented\")\n",
    "            else:\n",
    "                print(f\"  ‚ùå {description} - Not found\")\n",
    "                \n",
    "        # Extract model architecture details\n",
    "        print(\"\\nüèóÔ∏è  Model Architecture Details:\")\n",
    "        lines = model_code.split('\\n')\n",
    "        in_unet = False\n",
    "        indent_level = 0\n",
    "        \n",
    "        for line in lines:\n",
    "            if 'def unet' in line:\n",
    "                in_unet = True\n",
    "                print(\"  Main U-Net function found\")\n",
    "            elif in_unet and line.strip().startswith('def '):\n",
    "                break\n",
    "            elif in_unet and line.strip():\n",
    "                current_indent = len(line) - len(line.lstrip())\n",
    "                if 'conv' in line.lower() and 'conv2d' in line.lower():\n",
    "                    print(f\"    {'  ' * indent_level}üì¶ Convolutional Layer\")\n",
    "                elif 'maxpooling' in line.lower():\n",
    "                    print(f\"    {'  ' * indent_level}‚¨áÔ∏è  Downsampling (MaxPool)\")\n",
    "                    indent_level += 1\n",
    "                elif 'upsampling' in line.lower() or 'upconv' in line.lower():\n",
    "                    print(f\"    {'  ' * indent_level}‚¨ÜÔ∏è  Upsampling\")\n",
    "                    indent_level -= 1\n",
    "                elif 'concatenate' in line.lower():\n",
    "                    print(f\"    {'  ' * indent_level}üîó Skip Connection\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing model.py: {e}\")\n",
    "\n",
    "unet_code_reading()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c091be25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the U-Net architecture in more detail\n",
    "def detailed_architecture_analysis():\n",
    "    \"\"\"Detailed analysis of U-Net architecture implementation\"\"\"\n",
    "    \n",
    "    print(\"üî¨ Detailed U-Net Architecture Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Import and examine the model\n",
    "        from model import unet\n",
    "        \n",
    "        # Create a small model to examine structure\n",
    "        model = unet(input_size=(128, 128, 1))\n",
    "        \n",
    "        print(\"üìä Model Summary:\")\n",
    "        model.summary()\n",
    "        \n",
    "        print(\"\\nüèóÔ∏è  Layer-by-Layer Analysis:\")\n",
    "        for i, layer in enumerate(model.layers):\n",
    "            layer_type = layer.__class__.__name__\n",
    "            output_shape = layer.output_shape\n",
    "            print(f\"  {i:2d}. {layer_type:20} ‚Üí {str(output_shape)}\")\n",
    "            \n",
    "            # Highlight key U-Net components\n",
    "            if 'conv2d' in layer.name and 'concat' not in layer.name:\n",
    "                if 'up' in layer.name:\n",
    "                    print(f\"       ‚¨ÜÔ∏è  Decoder Convolution\")\n",
    "                else:\n",
    "                    print(f\"       ‚¨áÔ∏è  Encoder Convolution\")\n",
    "            elif 'max_pooling2d' in layer.name:\n",
    "                print(f\"       üîΩ Downsampling\")\n",
    "            elif 'up_sampling2d' in layer.name:\n",
    "                print(f\"       üîº Upsampling\") \n",
    "            elif 'concatenate' in layer.name:\n",
    "                print(f\"       üîó Skip Connection\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error in detailed analysis: {e}\")\n",
    "        \n",
    "        # Alternative analysis by reading code\n",
    "        print(\"\\nüìù Alternative Code Analysis:\")\n",
    "        try:\n",
    "            with open('model.py', 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                \n",
    "            encoder_blocks = 0\n",
    "            decoder_blocks = 0\n",
    "            skip_connections = 0\n",
    "            \n",
    "            for line in lines:\n",
    "                line_lower = line.lower()\n",
    "                if 'conv2d' in line_lower and 'maxpooling' not in line_lower:\n",
    "                    if 'up' in line_lower:\n",
    "                        decoder_blocks += 1\n",
    "                    else:\n",
    "                        encoder_blocks += 1\n",
    "                elif 'concatenate' in line_lower:\n",
    "                    skip_connections += 1\n",
    "                    \n",
    "            print(f\"  Encoder Blocks: {encoder_blocks}\")\n",
    "            print(f\"  Decoder Blocks: {decoder_blocks}\") \n",
    "            print(f\"  Skip Connections: {skip_connections}\")\n",
    "            print(f\"  U-Shape Verified: {encoder_blocks == decoder_blocks}\")\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"Could not perform code analysis: {e2}\")\n",
    "\n",
    "detailed_architecture_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8009e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive training and evaluation script\n",
    "def create_complete_pipeline():\n",
    "    \"\"\"Create complete training and evaluation pipeline\"\"\"\n",
    "    \n",
    "    print(\"üöÄ Creating Complete Training Pipeline\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    complete_script = '''\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from model import unet\n",
    "\n",
    "class CompleteTGSPipeline:\n",
    "    \"\"\"Complete pipeline for TGS Salt Identification with U-Net\"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=128):\n",
    "        self.img_size = img_size\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        \n",
    "    def prepare_data(self, data_dir='tgs_data'):\n",
    "        \"\"\"Prepare training and validation data\"\"\"\n",
    "        print(\"üìä Preparing data...\")\n",
    "        \n",
    "        # Load images and masks\n",
    "        images = []\n",
    "        masks = []\n",
    "        \n",
    "        image_dir = os.path.join(data_dir, 'train', 'images')\n",
    "        mask_dir = os.path.join(data_dir, 'train', 'masks')\n",
    "        \n",
    "        for img_file in os.listdir(image_dir):\n",
    "            if img_file.endswith('.png'):\n",
    "                # Load and resize image\n",
    "                img_path = os.path.join(image_dir, img_file)\n",
    "                img = plt.imread(img_path)\n",
    "                if len(img.shape) == 3:\n",
    "                    img = img[:, :, 0]  # Take first channel if RGB\n",
    "                img = np.array(Image.fromarray(img).resize((self.img_size, self.img_size)))\n",
    "                \n",
    "                # Load and resize mask\n",
    "                mask_path = os.path.join(mask_dir, img_file)\n",
    "                mask = plt.imread(mask_path)\n",
    "                if len(mask.shape) == 3:\n",
    "                    mask = mask[:, :, 0]\n",
    "                mask = np.array(Image.fromarray(mask).resize((self.img_size, self.img_size)))\n",
    "                \n",
    "                images.append(img)\n",
    "                masks.append(mask)\n",
    "        \n",
    "        # Convert to numpy arrays and normalize\n",
    "        images = np.array(images) / 255.0\n",
    "        masks = np.array(masks) / 255.0\n",
    "        \n",
    "        # Add channel dimension\n",
    "        images = images.reshape(-1, self.img_size, self.img_size, 1)\n",
    "        masks = masks.reshape(-1, self.img_size, self.img_size, 1)\n",
    "        \n",
    "        # Split into train and validation\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            images, masks, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Data prepared: {X_train.shape[0]} train, {X_val.shape[0]} validation\")\n",
    "        return X_train, X_val, y_train, y_val\n",
    "    \n",
    "    def build_model(self):\n",
    "        \"\"\"Build U-Net model\"\"\"\n",
    "        print(\"üèóÔ∏è  Building U-Net model...\")\n",
    "        \n",
    "        self.model = unet(input_size=(self.img_size, self.img_size, 1))\n",
    "        \n",
    "        # Custom compilation for salt segmentation\n",
    "        self.model.compile(\n",
    "            optimizer=Adam(lr=1e-4),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy', self.dice_coef, self.iou_score]\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Model built and compiled\")\n",
    "        return self.model\n",
    "    \n",
    "    def dice_coef(self, y_true, y_pred):\n",
    "        \"\"\"Dice coefficient metric\"\"\"\n",
    "        smooth = 1.0\n",
    "        y_true_f = tf.keras.backend.flatten(y_true)\n",
    "        y_pred_f = tf.keras.backend.flatten(y_pred)\n",
    "        intersection = tf.keras.backend.sum(y_true_f * y_pred_f)\n",
    "        return (2. * intersection + smooth) / (tf.keras.backend.sum(y_true_f) + tf.keras.backend.sum(y_pred_f) + smooth)\n",
    "    \n",
    "    def iou_score(self, y_true, y_pred):\n",
    "        \"\"\"Intersection over Union metric\"\"\"\n",
    "        smooth = 1.0\n",
    "        intersection = tf.keras.backend.sum(y_true * y_pred)\n",
    "        union = tf.keras.backend.sum(y_true) + tf.keras.backend.sum(y_pred) - intersection\n",
    "        return (intersection + smooth) / (union + smooth)\n",
    "    \n",
    "    def train(self, epochs=50, batch_size=16):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        print(\"üéØ Starting training...\")\n",
    "        \n",
    "        # Prepare data\n",
    "        X_train, X_val, y_train, y_val = self.prepare_data()\n",
    "        \n",
    "        # Build model\n",
    "        self.build_model()\n",
    "        \n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            ModelCheckpoint('best_tgs_unet.h5', monitor='val_loss', save_best_only=True),\n",
    "            EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-7)\n",
    "        ]\n",
    "        \n",
    "        # Train\n",
    "        self.history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Training completed!\")\n",
    "        return self.history\n",
    "    \n",
    "    def evaluate(self, test_dir='tgs_data/test'):\n",
    "        \"\"\"Evaluate model on test set\"\"\"\n",
    "        if self.model is None:\n",
    "            print(\"‚ùå Model not trained\")\n",
    "            return\n",
    "        \n",
    "        print(\"üìà Evaluating model...\")\n",
    "        \n",
    "        # Load test data\n",
    "        test_images = []\n",
    "        test_masks = []\n",
    "        \n",
    "        image_dir = os.path.join(test_dir, 'images')\n",
    "        mask_dir = os.path.join(test_dir, 'masks')\n",
    "        \n",
    "        for img_file in os.listdir(image_dir):\n",
    "            if img_file.endswith('.png'):\n",
    "                img_path = os.path.join(image_dir, img_file)\n",
    "                mask_path = os.path.join(mask_dir, img_file)\n",
    "                \n",
    "                img = plt.imread(img_path)\n",
    "                mask = plt.imread(mask_path)\n",
    "                \n",
    "                if len(img.shape) == 3:\n",
    "                    img = img[:, :, 0]\n",
    "                if len(mask.shape) == 3:\n",
    "                    mask = mask[:, :, 0]\n",
    "                \n",
    "                img = np.array(Image.fromarray(img).resize((self.img_size, self.img_size)))\n",
    "                mask = np.array(Image.fromarray(mask).resize((self.img_size, self.img_size)))\n",
    "                \n",
    "                test_images.append(img)\n",
    "                test_masks.append(mask)\n",
    "        \n",
    "        test_images = np.array(test_images) / 255.0\n",
    "        test_masks = np.array(test_masks) / 255.0\n",
    "        test_images = test_images.reshape(-1, self.img_size, self.img_size, 1)\n",
    "        test_masks = test_masks.reshape(-1, self.img_size, self.img_size, 1)\n",
    "        \n",
    "        # Evaluate\n",
    "        results = self.model.evaluate(test_images, test_masks, verbose=0)\n",
    "        metrics = ['Loss', 'Accuracy', 'Dice Coefficient', 'IoU Score']\n",
    "        \n",
    "        print(\"\\\\nüìä Test Results:\")\n",
    "        for metric, value in zip(metrics, results):\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def plot_results(self):\n",
    "        \"\"\"Plot training history and sample predictions\"\"\"\n",
    "        if self.history is None:\n",
    "            print(\"‚ùå No training history available\")\n",
    "            return\n",
    "        \n",
    "        # Plot training history\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Loss\n",
    "        axes[0, 0].plot(self.history.history['loss'], label='Training Loss')\n",
    "        axes[0, 0].plot(self.history.history['val_loss'], label='Validation Loss')\n",
    "        axes[0, 0].set_title('Model Loss')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].legend()\n",
    "        \n",
    "        # Accuracy\n",
    "        axes[0, 1].plot(self.history.history['accuracy'], label='Training Accuracy')\n",
    "        axes[0, 1].plot(self.history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        axes[0, 1].set_title('Model Accuracy')\n",
    "        axes[0, 1].set_ylabel('Accuracy')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].legend()\n",
    "        \n",
    "        # Dice Coefficient\n",
    "        if 'dice_coef' in self.history.history:\n",
    "            axes[1, 0].plot(self.history.history['dice_coef'], label='Training Dice')\n",
    "            axes[1, 0].plot(self.history.history['val_dice_coef'], label='Validation Dice')\n",
    "            axes[1, 0].set_title('Dice Coefficient')\n",
    "            axes[1, 0].set_ylabel('Dice')\n",
    "            axes[1, 0].set_xlabel('Epoch')\n",
    "            axes[1, 0].legend()\n",
    "        \n",
    "        # IoU Score\n",
    "        if 'iou_score' in self.history.history:\n",
    "            axes[1, 1].plot(self.history.history['iou_score'], label='Training IoU')\n",
    "            axes[1, 1].plot(self.history.history['val_iou_score'], label='Validation IoU')\n",
    "            axes[1, 1].set_title('IoU Score')\n",
    "            axes[1, 1].set_ylabel('IoU')\n",
    "            axes[1, 1].set_xlabel('Epoch')\n",
    "            axes[1, 1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Run complete pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    pipeline = CompleteTGSPipeline(img_size=128)\n",
    "    \n",
    "    # For demo purposes, we'll just show the structure\n",
    "    # In real scenario: pipeline.train(epochs=50)\n",
    "    print(\"üöÄ Complete pipeline ready!\")\n",
    "    print(\"To train: pipeline.train(epochs=50)\")\n",
    "    print(\"To evaluate: pipeline.evaluate()\")\n",
    "    print(\"To plot results: pipeline.plot_results()\")\n",
    "'''\n",
    "    \n",
    "    with open('complete_pipeline.py', 'w') as f:\n",
    "        f.write(complete_script)\n",
    "    \n",
    "    print(\"üìú Created complete pipeline: complete_pipeline.py\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Complete U-Net pipeline created!\")\n",
    "    print(\"\\\\nThis includes:\")\n",
    "    print(\"  ‚Ä¢ Data preparation and loading\")\n",
    "    print(\"  ‚Ä¢ U-Net model building\") \n",
    "    print(\"  ‚Ä¢ Training with callbacks\")\n",
    "    print(\"  ‚Ä¢ Evaluation with segmentation metrics\")\n",
    "    print(\"  ‚Ä¢ Visualization of results\")\n",
    "\n",
    "create_complete_pipeline()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

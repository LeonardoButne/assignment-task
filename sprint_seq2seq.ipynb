{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc6a0fbe",
   "metadata": {},
   "source": [
    "\n",
    "# 1. About this Sprint\n",
    "\n",
    "**Purpose of Sprint**  \n",
    "- Learn about applications for sequence (serial) data.  \n",
    "- Understand methodologies such as Sequence-to-Sequence and Image-to-Sequence.  \n",
    "- Apply these methods to machine translation and image captioning.  \n",
    "\n",
    "**How to learn**  \n",
    "- Study from publicly available code implementations.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9286a744",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Machine Translation\n",
    "\n",
    "A basic example of a methodology related to sequence data is **machine translation**.  \n",
    "This is achieved with the **Sequence to Sequence (Seq2Seq)** approach, where sequence data is used as input and sequence data is output.\n",
    "\n",
    "This method is widely used for:  \n",
    "- Translating between languages  \n",
    "- Summarizing sentences  \n",
    "- Generating automatic text (e.g., greetings)  \n",
    "\n",
    "We will use the code example:  \n",
    "[Hard/lstm_seq2seq.py](https://github.com/rstudio/hard/blob/master/lstm_seq2seq.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19160660",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: Import required libraries for Seq2Seq Machine Translation\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73b4f0f",
   "metadata": {},
   "source": [
    "\n",
    "### Code Reading (line by line explanation)\n",
    "\n",
    "- **Lines 51 to 55**: Importing Libraries (`numpy`, `keras.models`, `keras.layers`, etc.)  \n",
    "- **Lines 57 to 62**: Setting Hyperparameters (batch size, number of epochs, latent dimension)  \n",
    "- **Character-by-character tokenization**:  \n",
    "  Instead of treating words as tokens, this implementation treats **each character as a token**.  \n",
    "  This makes the model smaller and easier to train, but less semantically powerful.  \n",
    "\n",
    "ðŸ”¹ **Example with scikit-learn CountVectorizer**:  \n",
    "- `analyzer='char'`: Creates n-grams directly from characters.  \n",
    "- `analyzer='char_wb'`: Creates n-grams only within word boundaries.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2f678f",
   "metadata": {},
   "source": [
    "\n",
    "# 3. Image Captioning\n",
    "\n",
    "**Image captioning** is the task of generating descriptions for images.  \n",
    "It uses the **Image-to-Sequence** technique: input is an image, output is a text description.  \n",
    "\n",
    "Applications:  \n",
    "- Autonomous driving (describing the environment)  \n",
    "- Search engines (image tagging)  \n",
    "\n",
    "We will use the PyTorch implementation:  \n",
    "[yunjey/pytorch-tutorial - Image Captioning](https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/03-advanced/image_captioning)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8050b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: Testing a pretrained Image Captioning model in PyTorch\n",
    "# (using provided weights from the repo)\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Dummy example: preprocess image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "# In real execution: load pretrained model and generate caption\n",
    "print(\"Model would generate a caption for the given image here.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c2f91c",
   "metadata": {},
   "source": [
    "\n",
    "### Notes\n",
    "\n",
    "- This repo downloads pretrained weights and allows caption generation.  \n",
    "- Sometimes, the **default weight filename** in the code does not match the one downloaded â†’ must be renamed manually.  \n",
    "\n",
    "### Research: Running in Keras\n",
    "\n",
    "- Keras does not have an easy ready-made implementation for Image Captioning.  \n",
    "- To move from PyTorch to Keras:  \n",
    "  1. Define CNN encoder (e.g., VGG16, ResNet50).  \n",
    "  2. Define RNN/LSTM decoder.  \n",
    "  3. Handle embeddings for captions.  \n",
    "- For **weights transfer**, PyTorch `.pth` weights must be converted to NumPy and then loaded manually into a Keras model.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5706d9",
   "metadata": {},
   "source": [
    "\n",
    "# 4. Advanced Assignments\n",
    "\n",
    "### Code Reading and Rewriting\n",
    "\n",
    "- In PyTorch, the model is defined in **`model.py`**.  \n",
    "- If rewritten in Keras, we would define:  \n",
    "  - CNN Encoder (`keras.applications.VGG16` with `include_top=False`)  \n",
    "  - LSTM Decoder (`keras.layers.LSTM`)  \n",
    "  - Dense output layer with vocabulary size.  \n",
    "\n",
    "### Developmental Research\n",
    "\n",
    "1. **Translating into other languages**  \n",
    "   - Replace dataset with bilingual corpus (e.g., English â†” Japanese).  \n",
    "   - Retrain the Seq2Seq model.  \n",
    "\n",
    "2. **Evolving methods of machine translation**  \n",
    "   - Use **Attention mechanisms** (Bahdanau, Luong).  \n",
    "   - Use **Transformer models** (Vaswani et al., 2017).  \n",
    "   - Pretrained models: BERT, GPT, mBART, MarianMT.  \n",
    "\n",
    "3. **Generating images from text (opposite of Image Captioning)**  \n",
    "   - GANs (Generative Adversarial Networks).  \n",
    "   - DALLÂ·E, Stable Diffusion, Imagen.  \n",
    "   - Use text embeddings + image generators.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3031ae4",
   "metadata": {},
   "source": [
    "\n",
    "# 5. Conclusion\n",
    "\n",
    "- Sequence data has wide applications such as **machine translation** and **image captioning**.  \n",
    "- Machine translation may use **character-level tokenization**.  \n",
    "- Running pretrained models (like Image Captioning) requires attention to **weight filenames**.  \n",
    "- Transitioning to Keras requires manual weight handling from PyTorch.  \n",
    "- Evolutionary methods include **attention**, **transformers**, and even **text-to-image generation**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6510f2",
   "metadata": {},
   "source": [
    "\n",
    "# 6. References\n",
    "\n",
    "1. CountVectorizer â€” scikit-learn 0.21.3 documentation  \n",
    "   https://scikit-learn.org/0.21/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html  \n",
    "\n",
    "2. Ren, S., He, K., Girshick, R., Sun, J. (2015). Faster R-CNN.  \n",
    "   https://arxiv.org/pdf/1506.01497.pdf  \n",
    "\n",
    "3. Redmon, J., Farhadi, A. (2018). YOLOv3: An Incremental Improvement.  \n",
    "   https://pjreddie.com/media/files/papers/YOLOv3.pdf  \n",
    "\n",
    "4. Vaswani, A. et al. (2017). Attention is All You Need.  \n",
    "   https://arxiv.org/abs/1706.03762  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

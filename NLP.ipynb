{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6f5064e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ PROBLEM 1: Scratch BoW Implementation\n",
      "==================================================\n",
      "Input sentences:\n",
      "  1. This movie is SOOOO funny!!!\n",
      "  2. What a movie! I never\n",
      "  3. best movie ever!!!!! this movie\n",
      "\n",
      "ðŸ“Š 1-gram BoW Results:\n",
      "Vocabulary: ['a', 'best', 'ever', 'funny', 'i', 'is', 'movie', 'never', 'soooo', 'this', 'what']\n",
      "Sentence 1: [0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0]\n",
      "Sentence 2: [1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1]\n",
      "Sentence 3: [0, 1, 1, 0, 0, 0, 2, 0, 0, 1, 0]\n",
      "\n",
      "ðŸ“Š 2-gram BoW Results:\n",
      "Vocabulary: ['a_movie', 'best_movie', 'ever_this', 'i_never', 'is_soooo', 'movie_ever', 'movie_i', 'movie_is', 'soooo_funny', 'this_movie', 'what_a']\n",
      "Sentence 1: [0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0]\n",
      "Sentence 2: [1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
      "Sentence 3: [0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# PROBLEM 1: Scratch implementation of BoW\n",
    "def bow_scratch_implementation():\n",
    "    \"\"\"Implement BoW from scratch without scikit-learn\"\"\"\n",
    "    \n",
    "    print(\"ðŸŽ¯ PROBLEM 1: Scratch BoW Implementation\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # The three sentences from the problem\n",
    "    sentences = [\n",
    "        \"This movie is SOOOO funny!!!\",\n",
    "        \"What a movie! I never\", \n",
    "        \"best movie ever!!!!! this movie\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Input sentences:\")\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        print(f\"  {i+1}. {sentence}\")\n",
    "    \n",
    "    # 1-gram BoW implementation\n",
    "    def bow_1gram(sentences):\n",
    "        # Tokenize: split by space and remove punctuation\n",
    "        tokens_list = []\n",
    "        for sentence in sentences:\n",
    "            # Basic cleaning: lowercase and remove punctuation\n",
    "            cleaned = ''.join(char.lower() if char.isalnum() or char.isspace() else ' ' for char in sentence)\n",
    "            tokens = cleaned.split()\n",
    "            tokens_list.append(tokens)\n",
    "        \n",
    "        # Create vocabulary\n",
    "        vocabulary = set()\n",
    "        for tokens in tokens_list:\n",
    "            vocabulary.update(tokens)\n",
    "        vocabulary = sorted(list(vocabulary))\n",
    "        \n",
    "        # Create BoW vectors\n",
    "        bow_vectors = []\n",
    "        for tokens in tokens_list:\n",
    "            vector = [tokens.count(word) for word in vocabulary]\n",
    "            bow_vectors.append(vector)\n",
    "        \n",
    "        return bow_vectors, vocabulary\n",
    "    \n",
    "    # 2-gram BoW implementation  \n",
    "    def bow_2gram(sentences):\n",
    "        tokens_list = []\n",
    "        for sentence in sentences:\n",
    "            cleaned = ''.join(char.lower() if char.isalnum() or char.isspace() else ' ' for char in sentence)\n",
    "            tokens = cleaned.split()\n",
    "            tokens_list.append(tokens)\n",
    "        \n",
    "        # Create 2-grams\n",
    "        bigrams_list = []\n",
    "        for tokens in tokens_list:\n",
    "            bigrams = []\n",
    "            for i in range(len(tokens) - 1):\n",
    "                bigrams.append(f\"{tokens[i]}_{tokens[i+1]}\")\n",
    "            bigrams_list.append(bigrams)\n",
    "        \n",
    "        # Create vocabulary\n",
    "        vocabulary = set()\n",
    "        for bigrams in bigrams_list:\n",
    "            vocabulary.update(bigrams)\n",
    "        vocabulary = sorted(list(vocabulary))\n",
    "        \n",
    "        # Create BoW vectors\n",
    "        bow_vectors = []\n",
    "        for bigrams in bigrams_list:\n",
    "            vector = [bigrams.count(bigram) for bigram in vocabulary]\n",
    "            bow_vectors.append(vector)\n",
    "        \n",
    "        return bow_vectors, vocabulary\n",
    "    \n",
    "    # Calculate 1-gram and 2-gram\n",
    "    bow_1gram_vectors, vocab_1gram = bow_1gram(sentences)\n",
    "    bow_2gram_vectors, vocab_2gram = bow_2gram(sentences)\n",
    "    \n",
    "    print(\"\\nðŸ“Š 1-gram BoW Results:\")\n",
    "    print(f\"Vocabulary: {vocab_1gram}\")\n",
    "    for i, vector in enumerate(bow_1gram_vectors):\n",
    "        print(f\"Sentence {i+1}: {vector}\")\n",
    "    \n",
    "    print(\"\\nðŸ“Š 2-gram BoW Results:\")\n",
    "    print(f\"Vocabulary: {vocab_2gram}\")\n",
    "    for i, vector in enumerate(bow_2gram_vectors):\n",
    "        print(f\"Sentence {i+1}: {vector}\")\n",
    "    \n",
    "    return bow_1gram_vectors, vocab_1gram, bow_2gram_vectors, vocab_2gram\n",
    "\n",
    "# Run Problem 1\n",
    "bow_1gram_vectors, vocab_1gram, bow_2gram_vectors, vocab_2gram = bow_scratch_implementation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bbf815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "\n",
    "# Download stopwords if not available\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# PROBLEM 2: TF-IDF Calculation with scikit-learn\n",
    "def tfidf_calculation():\n",
    "    \"\"\"Calculate TF-IDF for IMDB dataset using scikit-learn\"\"\"\n",
    "    \n",
    "    print(\"ðŸŽ¯ PROBLEM 2: TF-IDF Calculation with scikit-learn\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Load IMDB dataset as instructed\n",
    "    train_review = load_files('./aclImdb/train/', encoding='utf-8')\n",
    "    x_train, y_train = train_review.data, train_review.target\n",
    "    \n",
    "    test_review = load_files('./aclImdb/test/', encoding='utf-8') \n",
    "    x_test, y_test = test_review.data, train_review.target\n",
    "    \n",
    "    print(f\"Training samples: {len(x_train)}\")\n",
    "    print(f\"Test samples: {len(x_test)}\")\n",
    "    print(f\"Labels: {train_review.target_names}\")\n",
    "    \n",
    "    # Get English stopwords from NLTK\n",
    "    stop_words = stopwords.words('english')\n",
    "    print(f\"Number of stopwords: {len(stop_words)}\")\n",
    "    print(f\"First 10 stopwords: {stop_words[:10]}\")\n",
    "    \n",
    "    # Create TF-IDF vectorizer with specified parameters\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        max_features=5000,           # Maximum vocabulary size\n",
    "        stop_words=stop_words,       # NLTK stop words\n",
    "        lowercase=True,              # Convert to lowercase\n",
    "        token_pattern=r'(?u)\\b\\w+\\b' # Token pattern\n",
    "    )\n",
    "    \n",
    "    # Fit and transform training data\n",
    "    print(\"\\nðŸ”„ Fitting TF-IDF vectorizer...\")\n",
    "    x_train_tfidf = tfidf_vectorizer.fit_transform(x_train)\n",
    "    x_test_tfidf = tfidf_vectorizer.transform(x_test)\n",
    "    \n",
    "    print(f\"TF-IDF matrix shape - Train: {x_train_tfidf.shape}\")\n",
    "    print(f\"TF-IDF matrix shape - Test: {x_test_tfidf.shape}\")\n",
    "    \n",
    "    # Show some feature names\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    print(f\"\\nFirst 20 feature names: {feature_names[:20]}\")\n",
    "    \n",
    "    # Show TF-IDF values for first sample\n",
    "    first_sample_tfidf = x_train_tfidf[0].toarray().flatten()\n",
    "    non_zero_indices = first_sample_tfidf.nonzero()[0]\n",
    "    \n",
    "    print(f\"\\nFirst sample - Non-zero TF-IDF values:\")\n",
    "    for idx in non_zero_indices[:10]:  # Show first 10 non-zero values\n",
    "        print(f\"  {feature_names[idx]}: {first_sample_tfidf[idx]:.4f}\")\n",
    "    \n",
    "    return x_train_tfidf, x_test_tfidf, y_train, y_test, tfidf_vectorizer\n",
    "\n",
    "# Run Problem 2\n",
    "x_train_tfidf, x_test_tfidf, y_train, y_test, tfidf_vectorizer = tfidf_calculation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df226090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 3: Learning with TF-IDF\n",
    "def classification_with_tfidf():\n",
    "    \"\"\"Train classifier using TF-IDF vectors\"\"\"\n",
    "    \n",
    "    print(\"ðŸŽ¯ PROBLEM 3: Classification with TF-IDF\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import accuracy_score, classification_report\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    \n",
    "    # Use Logistic Regression as binary classifier\n",
    "    classifier = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    \n",
    "    print(\"ðŸ”„ Training classifier...\")\n",
    "    classifier.fit(x_train_tfidf, y_train)\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred = classifier.predict(x_test_tfidf)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"âœ… Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))\n",
    "    \n",
    "    # Experiment with different parameters\n",
    "    print(\"\\nðŸ”¬ Experimenting with different parameters:\")\n",
    "    \n",
    "    parameters_experiment = [\n",
    "        {'max_features': 1000, 'ngram_range': (1, 1)},\n",
    "        {'max_features': 5000, 'ngram_range': (1, 1)}, \n",
    "        {'max_features': 5000, 'ngram_range': (1, 2)},\n",
    "        {'max_features': 10000, 'ngram_range': (1, 1)}\n",
    "    ]\n",
    "    \n",
    "    for params in parameters_experiment:\n",
    "        print(f\"\\nTesting: max_features={params['max_features']}, ngram_range={params['ngram_range']}\")\n",
    "        \n",
    "        # Create new vectorizer with different parameters\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            max_features=params['max_features'],\n",
    "            ngram_range=params['ngram_range'],\n",
    "            stop_words=stopwords.words('english'),\n",
    "            lowercase=True\n",
    "        )\n",
    "        \n",
    "        # Transform data\n",
    "        x_train_new = vectorizer.fit_transform(x_train)\n",
    "        x_test_new = vectorizer.transform(x_test)\n",
    "        \n",
    "        # Train and evaluate\n",
    "        clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "        clf.fit(x_train_new, y_train)\n",
    "        y_pred_new = clf.predict(x_test_new)\n",
    "        acc = accuracy_score(y_test, y_pred_new)\n",
    "        \n",
    "        print(f\"  Accuracy: {acc:.4f}\")\n",
    "    \n",
    "    return classifier\n",
    "\n",
    "# Run Problem 3  \n",
    "classifier = classification_with_tfidf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40cb05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 4: Scratch TF-IDF Implementation\n",
    "def tfidf_scratch_implementation():\n",
    "    \"\"\"Implement TF-IDF from scratch without scikit-learn\"\"\"\n",
    "    \n",
    "    print(\"ðŸŽ¯ PROBLEM 4: Scratch TF-IDF Implementation\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # The three sentences from the problem\n",
    "    sentences = [\n",
    "        \"This movie is SOOOO funny!!!\",\n",
    "        \"What a movie! I never\", \n",
    "        \"best movie ever!!!!! this movie\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Input sentences:\")\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        print(f\"  {i+1}. {sentence}\")\n",
    "    \n",
    "    def standard_tfidf(sentences):\n",
    "        \"\"\"Standard TF-IDF formula\"\"\"\n",
    "        # Tokenize sentences\n",
    "        tokenized_sentences = []\n",
    "        for sentence in sentences:\n",
    "            cleaned = ''.join(char.lower() if char.isalnum() or char.isspace() else ' ' for char in sentence)\n",
    "            tokens = cleaned.split()\n",
    "            tokenized_sentences.append(tokens)\n",
    "        \n",
    "        # Create vocabulary\n",
    "        vocabulary = set()\n",
    "        for tokens in tokenized_sentences:\n",
    "            vocabulary.update(tokens)\n",
    "        vocabulary = sorted(list(vocabulary))\n",
    "        \n",
    "        N = len(sentences)  # Number of documents\n",
    "        \n",
    "        # Calculate TF (Term Frequency) - standard formula\n",
    "        tf_matrix = []\n",
    "        for tokens in tokenized_sentences:\n",
    "            total_terms = len(tokens)\n",
    "            tf_vector = [tokens.count(term) / total_terms for term in vocabulary]\n",
    "            tf_matrix.append(tf_vector)\n",
    "        \n",
    "        # Calculate IDF (Inverse Document Frequency) - standard formula\n",
    "        idf_vector = []\n",
    "        for term in vocabulary:\n",
    "            doc_count = sum(1 for tokens in tokenized_sentences if term in tokens)\n",
    "            idf = np.log(N / doc_count) if doc_count > 0 else 0\n",
    "            idf_vector.append(idf)\n",
    "        \n",
    "        # Calculate TF-IDF\n",
    "        tfidf_matrix = []\n",
    "        for tf_vector in tf_matrix:\n",
    "            tfidf_vector = [tf * idf for tf, idf in zip(tf_vector, idf_vector)]\n",
    "            tfidf_matrix.append(tfidf_vector)\n",
    "        \n",
    "        return tfidf_matrix, vocabulary\n",
    "    \n",
    "    def sklearn_tfidf(sentences):\n",
    "        \"\"\"Scikit-learn TF-IDF formula\"\"\"\n",
    "        tokenized_sentences = []\n",
    "        for sentence in sentences:\n",
    "            cleaned = ''.join(char.lower() if char.isalnum() or char.isspace() else ' ' for char in sentence)\n",
    "            tokens = cleaned.split()\n",
    "            tokenized_sentences.append(tokens)\n",
    "        \n",
    "        vocabulary = set()\n",
    "        for tokens in tokenized_sentences:\n",
    "            vocabulary.update(tokens)\n",
    "        vocabulary = sorted(list(vocabulary))\n",
    "        \n",
    "        N = len(sentences)\n",
    "        \n",
    "        # TF: raw counts (same as BoW)\n",
    "        tf_matrix = []\n",
    "        for tokens in tokenized_sentences:\n",
    "            tf_vector = [tokens.count(term) for term in vocabulary]\n",
    "            tf_matrix.append(tf_vector)\n",
    "        \n",
    "        # IDF: scikit-learn formula\n",
    "        idf_vector = []\n",
    "        for term in vocabulary:\n",
    "            doc_count = sum(1 for tokens in tokenized_sentences if term in tokens)\n",
    "            idf = np.log((1 + N) / (1 + doc_count)) + 1\n",
    "            idf_vector.append(idf)\n",
    "        \n",
    "        # TF-IDF\n",
    "        tfidf_matrix = []\n",
    "        for tf_vector in tf_matrix:\n",
    "            tfidf_vector = [tf * idf for tf, idf in zip(tf_vector, idf_vector)]\n",
    "            tfidf_matrix.append(tfidf_vector)\n",
    "        \n",
    "        return tfidf_matrix, vocabulary\n",
    "    \n",
    "    # Calculate both versions\n",
    "    standard_tfidf_matrix, vocab_standard = standard_tfidf(sentences)\n",
    "    sklearn_tfidf_matrix, vocab_sklearn = sklearn_tfidf(sentences)\n",
    "    \n",
    "    print(\"\\nðŸ“Š Standard TF-IDF Results:\")\n",
    "    print(f\"Vocabulary: {vocab_standard}\")\n",
    "    for i, vector in enumerate(standard_tfidf_matrix):\n",
    "        print(f\"Sentence {i+1}: {[f'{val:.4f}' for val in vector]}\")\n",
    "    \n",
    "    print(\"\\nðŸ“Š Scikit-learn TF-IDF Results:\")\n",
    "    print(f\"Vocabulary: {vocab_sklearn}\")\n",
    "    for i, vector in enumerate(sklearn_tfidf_matrix):\n",
    "        print(f\"Sentence {i+1}: {[f'{val:.4f}' for val in vector]}\")\n",
    "    \n",
    "    return standard_tfidf_matrix, sklearn_tfidf_matrix\n",
    "\n",
    "# Run Problem 4\n",
    "import numpy as np\n",
    "standard_tfidf_matrix, sklearn_tfidf_matrix = tfidf_scratch_implementation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6e17a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 5: Preprocessing for Word2Vec\n",
    "def preprocess_corpus():\n",
    "    \"\"\"Preprocess IMDB corpus for Word2Vec training\"\"\"\n",
    "    \n",
    "    print(\"ðŸŽ¯ PROBLEM 5: Corpus Preprocessing for Word2Vec\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    import re\n",
    "    \n",
    "    # Load IMDB data\n",
    "    train_review = load_files('./aclImdb/train/', encoding='utf-8')\n",
    "    x_train = train_review.data\n",
    "    \n",
    "    print(f\"Original training samples: {len(x_train)}\")\n",
    "    \n",
    "    def preprocess_text(text):\n",
    "        \"\"\"Preprocess individual text: remove special chars, lowercase, tokenize\"\"\"\n",
    "        if isinstance(text, bytes):\n",
    "            text = text.decode('utf-8')\n",
    "        \n",
    "        # Remove special characters and URLs\n",
    "        text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove non-alphabetic characters\n",
    "        \n",
    "        # Convert to lowercase and split into tokens\n",
    "        tokens = text.lower().split()\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    # Preprocess all texts\n",
    "    processed_corpus = []\n",
    "    for i, text in enumerate(x_train[:1000]):  # Use first 1000 for demonstration\n",
    "        tokens = preprocess_text(text)\n",
    "        processed_corpus.append(tokens)\n",
    "        \n",
    "        if i < 3:  # Show first 3 examples\n",
    "            print(f\"\\nSample {i+1}:\")\n",
    "            print(f\"  Original: {text[:100]}...\")\n",
    "            print(f\"  Processed: {tokens[:10]}...\")\n",
    "    \n",
    "    print(f\"\\nâœ… Preprocessed corpus: {len(processed_corpus)} documents\")\n",
    "    print(f\"Total tokens in first document: {len(processed_corpus[0])}\")\n",
    "    \n",
    "    return processed_corpus\n",
    "\n",
    "# Run Problem 5\n",
    "processed_corpus = preprocess_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec60f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 6: Word2Vec Training\n",
    "def train_word2vec():\n",
    "    \"\"\"Train Word2Vec model on preprocessed corpus\"\"\"\n",
    "    \n",
    "    print(\"ðŸŽ¯ PROBLEM 6: Word2Vec Training\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    from gensim.models import Word2Vec\n",
    "    \n",
    "    # Train Word2Vec model\n",
    "    print(\"ðŸ”„ Training Word2Vec model...\")\n",
    "    model = Word2Vec(\n",
    "        sentences=processed_corpus,\n",
    "        vector_size=100,      # Dimension of word vectors\n",
    "        window=5,            # Context window size\n",
    "        min_count=5,         # Ignore words with lower frequency\n",
    "        workers=4,           # Number of CPU cores\n",
    "        sg=1                 # Skip-gram (1) vs CBOW (0)\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Word2Vec model trained successfully!\")\n",
    "    print(f\"Vocabulary size: {len(model.wv.key_to_index)}\")\n",
    "    print(f\"Vector dimension: {model.vector_size}\")\n",
    "    \n",
    "    # Show some words from vocabulary\n",
    "    vocab_words = list(model.wv.key_to_index.keys())[:20]\n",
    "    print(f\"Sample vocabulary: {vocab_words}\")\n",
    "    \n",
    "    # Test word similarity\n",
    "    test_words = ['movie', 'good', 'bad', 'story']\n",
    "    for word in test_words:\n",
    "        if word in model.wv.key_to_index:\n",
    "            print(f\"\\nWords similar to '{word}':\")\n",
    "            similar_words = model.wv.most_similar(word, topn=5)\n",
    "            for similar, score in similar_words:\n",
    "                print(f\"  {similar}: {score:.4f}\")\n",
    "        else:\n",
    "            print(f\"'{word}' not in vocabulary\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Run Problem 6\n",
    "word2vec_model = train_word2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48811ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 7: Vector Visualization\n",
    "def visualize_word_vectors():\n",
    "    \"\"\"Visualize word vectors using t-SNE\"\"\"\n",
    "    \n",
    "    print(\"ðŸŽ¯ PROBLEM 7: Word Vector Visualization\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    from sklearn.manifold import TSNE\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Get words and vectors\n",
    "    words = list(word2vec_model.wv.key_to_index.keys())[:50]  # First 50 words\n",
    "    word_vectors = [word2vec_model.wv[word] for word in words]\n",
    "    \n",
    "    # Apply t-SNE for 2D visualization\n",
    "    print(\"ðŸ”„ Applying t-SNE dimensionality reduction...\")\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=15)\n",
    "    vectors_2d = tsne.fit_transform(word_vectors)\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], alpha=0.7)\n",
    "    \n",
    "    # Add word labels\n",
    "    for i, word in enumerate(words):\n",
    "        plt.annotate(word, xy=(vectors_2d[i, 0], vectors_2d[i, 1]), \n",
    "                    xytext=(5, 2), textcoords='offset points',\n",
    "                    fontsize=8, alpha=0.8)\n",
    "    \n",
    "    plt.title('Word2Vec Vector Visualization (t-SNE)')\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Find similar words for specific examples\n",
    "    test_words = ['good', 'bad', 'movie', 'story']\n",
    "    for word in test_words:\n",
    "        if word in word2vec_model.wv.key_to_index:\n",
    "            print(f\"\\nMost similar to '{word}':\")\n",
    "            similar = word2vec_model.wv.most_similar(word, topn=5)\n",
    "            for sim_word, score in similar:\n",
    "                print(f\"  {sim_word}: {score:.4f}\")\n",
    "\n",
    "# Run Problem 7\n",
    "visualize_word_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c27f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 8: Classification with Word2Vec\n",
    "def classification_with_word2vec():\n",
    "    \"\"\"Classify IMDB reviews using Word2Vec vectors\"\"\"\n",
    "    \n",
    "    print(\"ðŸŽ¯ PROBLEM 8: Classification with Word2Vec\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    import numpy as np\n",
    "    \n",
    "    # Load data\n",
    "    train_review = load_files('./aclImdb/train/', encoding='utf-8')\n",
    "    x_train, y_train = train_review.data[:1000], train_review.target[:1000]  # Use subset\n",
    "    \n",
    "    # Convert documents to vectors by averaging word vectors\n",
    "    def document_to_vector(text, model):\n",
    "        tokens = preprocess_text(text)\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in model.wv.key_to_index:\n",
    "                vectors.append(model.wv[token])\n",
    "        \n",
    "        if len(vectors) > 0:\n",
    "            return np.mean(vectors, axis=0)\n",
    "        else:\n",
    "            return np.zeros(model.vector_size)\n",
    "    \n",
    "    # Convert training data to vectors\n",
    "    print(\"ðŸ”„ Converting documents to Word2Vec vectors...\")\n",
    "    x_train_vectors = []\n",
    "    for text in x_train:\n",
    "        vector = document_to_vector(text, word2vec_model)\n",
    "        x_train_vectors.append(vector)\n",
    "    \n",
    "    x_train_vectors = np.array(x_train_vectors)\n",
    "    \n",
    "    print(f\"Training vectors shape: {x_train_vectors.shape}\")\n",
    "    \n",
    "    # Train classifier\n",
    "    print(\"ðŸ”„ Training classifier...\")\n",
    "    classifier = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    classifier.fit(x_train_vectors, y_train)\n",
    "    \n",
    "    # Simple train accuracy (for demonstration)\n",
    "    train_pred = classifier.predict(x_train_vectors)\n",
    "    train_accuracy = accuracy_score(y_train, train_pred)\n",
    "    \n",
    "    print(f\"âœ… Training Accuracy: {train_accuracy:.4f}\")\n",
    "    \n",
    "    # Compare with pre-trained vectors (conceptual)\n",
    "    print(\"\\nðŸ’¡ Using pre-trained vectors (conceptual):\")\n",
    "    print(\"  â€¢ Download pre-trained Word2Vec/FastText/GloVe vectors\")\n",
    "    print(\"  â€¢ Map words to pre-trained vectors instead of training\")\n",
    "    print(\"  â€¢ Often better performance with large pre-trained models\")\n",
    "    print(\"  â€¢ Examples: Google News Word2Vec, FastText Wikipedia vectors\")\n",
    "    \n",
    "    return classifier, x_train_vectors\n",
    "\n",
    "# Run Problem 8\n",
    "classifier_w2v, x_train_vectors = classification_with_word2vec()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baced93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Verificar versão do TensorFlow\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Assignment - TensorFlow Framework Analysis\n",
    "# \n",
    "# ## Question 1: Review of components needed to implement deep learning\n",
    "# \n",
    "# Based on what we learned implementing neural networks from scratch, the main components are:\n",
    "# \n",
    "# 1. **Weight and bias initialization**\n",
    "# 2. **Epoch loop**\n",
    "# 3. **Forward propagation**\n",
    "# 4. **Loss function calculation**\n",
    "# 5. **Backward propagation**\n",
    "# 6. **Parameter update (optimization)**\n",
    "# 7. **Evaluation metrics calculation**\n",
    "# 8. **Dataset splitting (train/val/test)**\n",
    "# 9. **Mini-batch processing**\n",
    "# 10. **Activation functions**\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Question 2: Correspondence between scratch implementation and TensorFlow\n",
    "# \n",
    "# Let's analyze how TensorFlow implements each component:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Verificar versão do TensorFlow\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Análise do código sample - Correspondências:\n",
    "# \n",
    "# 1. **Inicialização de pesos**: `tf.Variable(tf.random_normal(...))`\n",
    "# 2. **Loop de épocas**: `for epoch in range(num_epochs)`\n",
    "# 3. **Forward propagation**: Função `example_net()` com operações da rede\n",
    "# 4. **Função de perda**: `tf.nn.sigmoid_cross_entropy_with_logits()`\n",
    "# 5. **Backward propagation**: `optimizer.minimize(loss_op)` (automático)\n",
    "# 6. **Atualização de parâmetros**: `train_op` no `sess.run()`\n",
    "# 7. **Métricas**: `accuracy` calculada com `tf.equal()` e `tf.reduce_mean()`\n",
    "# 8. **Divisão do dataset**: `train_test_split()`\n",
    "# 9. **Mini-batch**: Classe `GetMiniBatch`\n",
    "# 10. **Funções de ativação**: `tf.nn.relu()`, `tf.sigmoid()`\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Questão 3: Modelo Iris com 3 classes\n",
    "\n",
    "# %%\n",
    "# Configuração para dataset Iris com 3 classes\n",
    "print(\"=== IRIS 3 CLASSES ===\")\n",
    "\n",
    "# Carregar dados\n",
    "df_iris = pd.read_csv(\"Iris.csv\")\n",
    "print(f\"Dataset shape: {df_iris.shape}\")\n",
    "print(f\"Species: {df_iris['Species'].unique()}\")\n",
    "\n",
    "# Preparar features e target\n",
    "X_iris = df_iris.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]].values\n",
    "y_iris = df_iris[\"Species\"]\n",
    "\n",
    "# Converter labels para numérico (3 classes)\n",
    "species_map = {'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2}\n",
    "y_iris = y_iris.map(species_map).values\n",
    "y_iris = y_iris.astype(np.int64)[:, np.newaxis]\n",
    "\n",
    "print(f\"X shape: {X_iris.shape}\")\n",
    "print(f\"y shape: {y_iris.shape}\")\n",
    "print(f\"Class distribution: {np.unique(y_iris, return_counts=True)}\")\n",
    "\n",
    "# Dividir dados\n",
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.2, random_state=0, stratify=y_iris\n",
    ")\n",
    "X_train_iris, X_val_iris, y_train_iris, y_val_iris = train_test_split(\n",
    "    X_train_iris, y_train_iris, test_size=0.2, random_state=0, stratify=y_train_iris\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train_iris.shape}, Val: {X_val_iris.shape}, Test: {X_test_iris.shape}\")\n",
    "\n",
    "# %%\n",
    "class MultiClassNet:\n",
    "    \"\"\"\n",
    "    Rede neural para classificação multi-classe\n",
    "    \"\"\"\n",
    "    def __init__(self, n_input, n_hidden1=50, n_hidden2=100, n_classes=3, learning_rate=0.001):\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden1 = n_hidden1\n",
    "        self.n_hidden2 = n_hidden2\n",
    "        self.n_classes = n_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Placeholders\n",
    "        self.X = tf.placeholder(tf.float32, [None, n_input])\n",
    "        self.Y = tf.placeholder(tf.int64, [None, 1])\n",
    "        \n",
    "        # Converter labels para one-hot encoding\n",
    "        self.Y_one_hot = tf.one_hot(tf.squeeze(self.Y), depth=n_classes)\n",
    "        \n",
    "        # Build network\n",
    "        self.logits = self._build_network()\n",
    "        \n",
    "        # Loss e optimizer\n",
    "        self.loss_op = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.Y_one_hot, logits=self.logits)\n",
    "        )\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        self.train_op = self.optimizer.minimize(self.loss_op)\n",
    "        \n",
    "        # Predictions e accuracy\n",
    "        self.predictions = tf.argmax(self.logits, axis=1)\n",
    "        self.correct_pred = tf.equal(self.predictions, tf.squeeze(self.Y))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_pred, tf.float32))\n",
    "    \n",
    "    def _build_network(self):\n",
    "        \"\"\"Construir a arquitetura da rede\"\"\"\n",
    "        tf.set_random_seed(0)\n",
    "        \n",
    "        # Camadas\n",
    "        weights = {\n",
    "            'w1': tf.Variable(tf.random_normal([self.n_input, self.n_hidden1])),\n",
    "            'w2': tf.Variable(tf.random_normal([self.n_hidden1, self.n_hidden2])),\n",
    "            'w3': tf.Variable(tf.random_normal([self.n_hidden2, self.n_classes]))\n",
    "        }\n",
    "        biases = {\n",
    "            'b1': tf.Variable(tf.random_normal([self.n_hidden1])),\n",
    "            'b2': tf.Variable(tf.random_normal([self.n_hidden2])),\n",
    "            'b3': tf.Variable(tf.random_normal([self.n_classes]))\n",
    "        }\n",
    "        \n",
    "        layer_1 = tf.add(tf.matmul(self.X, weights['w1']), biases['b1'])\n",
    "        layer_1 = tf.nn.relu(layer_1)\n",
    "        layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "        layer_2 = tf.nn.relu(layer_2)\n",
    "        layer_output = tf.add(tf.matmul(layer_2, weights['w3']), biases['b3'])\n",
    "        \n",
    "        return layer_output\n",
    "\n",
    "# %%\n",
    "# Treinar modelo Iris 3 classes\n",
    "print(\"Treinando modelo Iris 3 classes...\")\n",
    "\n",
    "# Hiperparâmetros\n",
    "learning_rate = 0.001\n",
    "batch_size = 10\n",
    "num_epochs = 100\n",
    "\n",
    "# Criar modelo\n",
    "model_iris = MultiClassNet(\n",
    "    n_input=X_train_iris.shape[1],\n",
    "    n_classes=3,\n",
    "    learning_rate=learning_rate\n",
    ")\n",
    "\n",
    "# Inicializar variáveis\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Data iterator\n",
    "get_mini_batch_train = GetMiniBatch(X_train_iris, y_train_iris, batch_size=batch_size)\n",
    "\n",
    "# Treinar\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_batch = len(get_mini_batch_train)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        \n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            _, loss, acc = sess.run([model_iris.train_op, model_iris.loss_op, model_iris.accuracy],\n",
    "                                  feed_dict={model_iris.X: mini_batch_x, model_iris.Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        \n",
    "        avg_loss = total_loss / total_batch\n",
    "        avg_acc = total_acc / total_batch\n",
    "        \n",
    "        # Validation\n",
    "        val_loss, val_acc = sess.run([model_iris.loss_op, model_iris.accuracy],\n",
    "                                   feed_dict={model_iris.X: X_val_iris, model_iris.Y: y_val_iris})\n",
    "        \n",
    "        train_losses.append(avg_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(avg_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Acc: {avg_acc:.3f}, Val Acc: {val_acc:.3f}\")\n",
    "    \n",
    "    # Test\n",
    "    test_acc = sess.run(model_iris.accuracy,\n",
    "                       feed_dict={model_iris.X: X_test_iris, model_iris.Y: y_test_iris})\n",
    "    print(f\"\\nTest Accuracy: {test_acc:.3f}\")\n",
    "\n",
    "# %%\n",
    "# Plot resultados Iris 3 classes\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.title('Loss - Iris 3 Classes')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accs, label='Train Accuracy')\n",
    "plt.plot(val_accs, label='Val Accuracy')\n",
    "plt.title('Accuracy - Iris 3 Classes')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Questão 4: Modelo House Prices (Regressão)\n",
    "\n",
    "# %%\n",
    "print(\"\\n=== HOUSE PRICES REGRESSION ===\")\n",
    "\n",
    "# Carregar dados House Prices\n",
    "try:\n",
    "    df_house = pd.read_csv(\"train.csv\")\n",
    "    print(f\"House Prices dataset shape: {df_house.shape}\")\n",
    "    \n",
    "    # Selecionar features e target\n",
    "    features = ['GrLivArea', 'YearBuilt', 'OverallQual', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF']\n",
    "    target = 'SalePrice'\n",
    "    \n",
    "    # Verificar se colunas existem\n",
    "    available_features = [f for f in features if f in df_house.columns]\n",
    "    print(f\"Available features: {available_features}\")\n",
    "    \n",
    "    if target not in df_house.columns:\n",
    "        raise ValueError(f\"Target column '{target}' not found\")\n",
    "    \n",
    "    # Preparar dados\n",
    "    X_house = df_house[available_features].fillna(0).values\n",
    "    y_house = df_house[target].values[:, np.newaxis]\n",
    "    \n",
    "    print(f\"X shape: {X_house.shape}, y shape: {y_house.shape}\")\n",
    "    print(f\"Price stats - Min: {y_house.min():.0f}, Max: {y_house.max():.0f}, Mean: {y_house.mean():.0f}\")\n",
    "    \n",
    "    # Normalizar features e target\n",
    "    X_house = (X_house - X_house.mean(axis=0)) / (X_house.std(axis=0) + 1e-8)\n",
    "    y_house = (y_house - y_house.mean()) / (y_house.std() + 1e-8)\n",
    "    \n",
    "    # Dividir dados\n",
    "    X_train_house, X_test_house, y_train_house, y_test_house = train_test_split(\n",
    "        X_house, y_house, test_size=0.2, random_state=0\n",
    "    )\n",
    "    X_train_house, X_val_house, y_train_house, y_val_house = train_test_split(\n",
    "        X_train_house, y_train_house, test_size=0.2, random_state=0\n",
    "    )\n",
    "    \n",
    "    print(f\"Train: {X_train_house.shape}, Val: {X_val_house.shape}, Test: {X_test_house.shape}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"House Prices dataset not found. Using synthetic data for demonstration.\")\n",
    "    \n",
    "    # Criar dados sintéticos\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    X_house = np.random.randn(n_samples, 3)\n",
    "    true_weights = np.array([2.5, 1.8, -0.9])[:, np.newaxis]\n",
    "    y_house = X_house @ true_weights + np.random.randn(n_samples, 1) * 0.5\n",
    "    \n",
    "    # Dividir dados\n",
    "    X_train_house, X_test_house, y_train_house, y_test_house = train_test_split(\n",
    "        X_house, y_house, test_size=0.2, random_state=0\n",
    "    )\n",
    "    X_train_house, X_val_house, y_train_house, y_val_house = train_test_split(\n",
    "        X_train_house, y_train_house, test_size=0.2, random_state=0\n",
    "    )\n",
    "    \n",
    "    print(f\"Synthetic data - Train: {X_train_house.shape}, Val: {X_val_house.shape}, Test: {X_test_house.shape}\")\n",
    "\n",
    "# %%\n",
    "class RegressionNet:\n",
    "    \"\"\"\n",
    "    Rede neural para problemas de regressão\n",
    "    \"\"\"\n",
    "    def __init__(self, n_input, n_hidden1=50, n_hidden2=100, learning_rate=0.001):\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden1 = n_hidden1\n",
    "        self.n_hidden2 = n_hidden2\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Placeholders\n",
    "        self.X = tf.placeholder(tf.float32, [None, n_input])\n",
    "        self.Y = tf.placeholder(tf.float32, [None, 1])\n",
    "        \n",
    "        # Build network\n",
    "        self.predictions = self._build_network()\n",
    "        \n",
    "        # Loss (MSE) e optimizer\n",
    "        self.loss_op = tf.reduce_mean(tf.square(self.Y - self.predictions))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        self.train_op = self.optimizer.minimize(self.loss_op)\n",
    "        \n",
    "        # Métricas adicionais\n",
    "        self.mae = tf.reduce_mean(tf.abs(self.Y - self.predictions))\n",
    "        self.rmse = tf.sqrt(self.loss_op)\n",
    "    \n",
    "    def _build_network(self):\n",
    "        \"\"\"Construir a arquitetura da rede\"\"\"\n",
    "        tf.set_random_seed(0)\n",
    "        \n",
    "        weights = {\n",
    "            'w1': tf.Variable(tf.random_normal([self.n_input, self.n_hidden1])),\n",
    "            'w2': tf.Variable(tf.random_normal([self.n_hidden1, self.n_hidden2])),\n",
    "            'w3': tf.Variable(tf.random_normal([self.n_hidden2, 1]))\n",
    "        }\n",
    "        biases = {\n",
    "            'b1': tf.Variable(tf.random_normal([self.n_hidden1])),\n",
    "            'b2': tf.Variable(tf.random_normal([self.n_hidden2])),\n",
    "            'b3': tf.Variable(tf.random_normal([1]))\n",
    "        }\n",
    "        \n",
    "        layer_1 = tf.add(tf.matmul(self.X, weights['w1']), biases['b1'])\n",
    "        layer_1 = tf.nn.relu(layer_1)\n",
    "        layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "        layer_2 = tf.nn.relu(layer_2)\n",
    "        layer_output = tf.add(tf.matmul(layer_2, weights['w3']), biases['b3'])\n",
    "        \n",
    "        return layer_output\n",
    "\n",
    "# %%\n",
    "# Treinar modelo House Prices\n",
    "print(\"Treinando modelo House Prices...\")\n",
    "\n",
    "# Hiperparâmetros\n",
    "learning_rate = 0.001\n",
    "batch_size = 16\n",
    "num_epochs = 100\n",
    "\n",
    "# Criar modelo\n",
    "model_house = RegressionNet(\n",
    "    n_input=X_train_house.shape[1],\n",
    "    learning_rate=learning_rate\n",
    ")\n",
    "\n",
    "# Inicializar variáveis\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Data iterator\n",
    "get_mini_batch_house = GetMiniBatch(X_train_house, y_train_house, batch_size=batch_size)\n",
    "\n",
    "# Treinar\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_batch = len(get_mini_batch_house)\n",
    "        total_loss = 0\n",
    "        \n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_house):\n",
    "            _, loss = sess.run([model_house.train_op, model_house.loss_op],\n",
    "                             feed_dict={model_house.X: mini_batch_x, model_house.Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "        \n",
    "        avg_loss = total_loss / total_batch\n",
    "        \n",
    "        # Validation\n",
    "        val_loss, val_mae, val_rmse = sess.run([model_house.loss_op, model_house.mae, model_house.rmse],\n",
    "                                              feed_dict={model_house.X: X_val_house, model_house.Y: y_val_house})\n",
    "        \n",
    "        train_losses.append(avg_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch}, Train Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val MAE: {val_mae:.4f}, Val RMSE: {val_rmse:.4f}\")\n",
    "    \n",
    "    # Test\n",
    "    test_loss, test_mae, test_rmse = sess.run([model_house.loss_op, model_house.mae, model_house.rmse],\n",
    "                                             feed_dict={model_house.X: X_test_house, model_house.Y: y_test_house})\n",
    "    print(f\"\\nTest Results - Loss: {test_loss:.4f}, MAE: {test_mae:.4f}, RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "# %%\n",
    "# Plot resultados House Prices\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss (MSE)')\n",
    "plt.plot(val_losses, label='Val Loss (MSE)')\n",
    "plt.title('Loss - House Prices')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Questão 5: Modelo MNIST\n",
    "\n",
    "# %%\n",
    "print(\"\\n=== MNIST DATASET ===\")\n",
    "\n",
    "# Carregar MNIST dataset\n",
    "try:\n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "    mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=False)\n",
    "    \n",
    "    X_train_mnist = mnist.train.images\n",
    "    y_train_mnist = mnist.train.labels[:, np.newaxis]\n",
    "    X_val_mnist = mnist.validation.images\n",
    "    y_val_mnist = mnist.validation.labels[:, np.newaxis]\n",
    "    X_test_mnist = mnist.test.images\n",
    "    y_test_mnist = mnist.test.labels[:, np.newaxis]\n",
    "    \n",
    "    print(f\"MNIST shapes:\")\n",
    "    print(f\"Train: {X_train_mnist.shape}, {y_train_mnist.shape}\")\n",
    "    print(f\"Val: {X_val_mnist.shape}, {y_val_mnist.shape}\")\n",
    "    print(f\"Test: {X_test_mnist.shape}, {y_test_mnist.shape}\")\n",
    "    print(f\"Pixel range: [{X_train_mnist.min():.3f}, {X_train_mnist.max():.3f}]\")\n",
    "\n",
    "except:\n",
    "    print(\"MNIST dataset not available. Using Iris 3-class as alternative.\")\n",
    "    # Usar Iris 3-class como fallback\n",
    "    X_train_mnist, X_test_mnist, y_train_mnist, y_test_mnist = X_train_iris, X_test_iris, y_train_iris, y_test_iris\n",
    "    X_train_mnist, X_val_mnist, y_train_mnist, y_val_mnist = X_train_iris, X_val_iris, y_train_iris, y_val_iris\n",
    "\n",
    "# %%\n",
    "class MNISTNet:\n",
    "    \"\"\"\n",
    "    Rede neural para MNIST (simulando a implementação scratch)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_input, n_hidden1=200, n_hidden2=100, n_classes=10, learning_rate=0.001):\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden1 = n_hidden1\n",
    "        self.n_hidden2 = n_hidden2\n",
    "        self.n_classes = n_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Placeholders\n",
    "        self.X = tf.placeholder(tf.float32, [None, n_input])\n",
    "        self.Y = tf.placeholder(tf.int64, [None, 1])\n",
    "        \n",
    "        # Converter labels para one-hot\n",
    "        self.Y_one_hot = tf.one_hot(tf.squeeze(self.Y), depth=n_classes)\n",
    "        \n",
    "        # Build network (similar à implementação scratch)\n",
    "        self.logits = self._build_network()\n",
    "        \n",
    "        # Loss e optimizer\n",
    "        self.loss_op = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.Y_one_hot, logits=self.logits)\n",
    "        )\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        self.train_op = self.optimizer.minimize(self.loss_op)\n",
    "        \n",
    "        # Predictions e accuracy\n",
    "        self.predictions = tf.argmax(self.logits, axis=1)\n",
    "        self.correct_pred = tf.equal(self.predictions, tf.squeeze(self.Y))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_pred, tf.float32))\n",
    "    \n",
    "    def _build_network(self):\n",
    "        \"\"\"Construir arquitetura similar à implementação scratch\"\"\"\n",
    "        tf.set_random_seed(0)\n",
    "        \n",
    "        # Inicialização similar à scratch\n",
    "        weights = {\n",
    "            'w1': tf.Variable(tf.random_normal([self.n_input, self.n_hidden1], stddev=0.1)),\n",
    "            'w2': tf.Variable(tf.random_normal([self.n_hidden1, self.n_hidden2], stddev=0.1)),\n",
    "            'w3': tf.Variable(tf.random_normal([self.n_hidden2, self.n_classes], stddev=0.1))\n",
    "        }\n",
    "        biases = {\n",
    "            'b1': tf.Variable(tf.zeros([self.n_hidden1])),\n",
    "            'b2': tf.Variable(tf.zeros([self.n_hidden2])),\n",
    "            'b3': tf.Variable(tf.zeros([self.n_classes]))\n",
    "        }\n",
    "        \n",
    "        # Forward pass (similar à scratch)\n",
    "        layer_1 = tf.add(tf.matmul(self.X, weights['w1']), biases['b1'])\n",
    "        layer_1 = tf.nn.sigmoid(layer_1)  # Usando sigmoid como na implementação scratch\n",
    "        \n",
    "        layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "        layer_2 = tf.nn.sigmoid(layer_2)\n",
    "        \n",
    "        layer_output = tf.add(tf.matmul(layer_2, weights['w3']), biases['b3'])\n",
    "        \n",
    "        return layer_output\n",
    "\n",
    "# %%\n",
    "# Treinar modelo MNIST\n",
    "print(\"Treinando modelo MNIST...\")\n",
    "\n",
    "# Hiperparâmetros\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "num_epochs = 50\n",
    "\n",
    "# Criar modelo\n",
    "model_mnist = MNISTNet(\n",
    "    n_input=X_train_mnist.shape[1],\n",
    "    n_classes=len(np.unique(y_train_mnist)),\n",
    "    learning_rate=learning_rate\n",
    ")\n",
    "\n",
    "# Inicializar variáveis\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Data iterator\n",
    "get_mini_batch_mnist = GetMiniBatch(X_train_mnist, y_train_mnist, batch_size=batch_size)\n",
    "\n",
    "# Treinar\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_batch = len(get_mini_batch_mnist)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        \n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_mnist):\n",
    "            _, loss, acc = sess.run([model_mnist.train_op, model_mnist.loss_op, model_mnist.accuracy],\n",
    "                                  feed_dict={model_mnist.X: mini_batch_x, model_mnist.Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        \n",
    "        avg_loss = total_loss / total_batch\n",
    "        avg_acc = total_acc / total_batch\n",
    "        \n",
    "        # Validation\n",
    "        val_loss, val_acc = sess.run([model_mnist.loss_op, model_mnist.accuracy],\n",
    "                                   feed_dict={model_mnist.X: X_val_mnist, model_mnist.Y: y_val_mnist})\n",
    "        \n",
    "        train_losses.append(avg_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(avg_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Acc: {avg_acc:.3f}, Val Acc: {val_acc:.3f}\")\n",
    "    \n",
    "    # Test\n",
    "    test_acc = sess.run(model_mnist.accuracy,\n",
    "                       feed_dict={model_mnist.X: X_test_mnist, model_mnist.Y: y_test_mnist})\n",
    "    print(f\"\\nTest Accuracy: {test_acc:.3f}\")\n",
    "\n",
    "# %%\n",
    "# Plot resultados MNIST\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.title('Loss - MNIST')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accs, label='Train Accuracy')\n",
    "plt.plot(val_accs, label='Val Accuracy')\n",
    "plt.title('Accuracy - MNIST')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Resumo das Diferenças entre Implementações\n",
    "# \n",
    "# ### Classificação Binária vs Multi-classe:\n",
    "# - **Binária**: `sigmoid_cross_entropy_with_logits`, saída com 1 neurônio\n",
    "# - **Multi-classe**: `softmax_cross_entropy_with_logits_v2`, saída com n_classes neurônios\n",
    "# \n",
    "# ### Classificação vs Regressão:\n",
    "# - **Classificação**: Cross-entropy loss, métricas de accuracy\n",
    "# - **Regressão**: MSE loss, métricas como MAE e RMSE\n",
    "# \n",
    "# ### Framework vs Scratch:\n",
    "# - **TensorFlow**: Gerencia automaticamente backward propagation, oferece otimizadores prontos\n",
    "# - **Scratch**: Implementação manual de todas as operações\n",
    "\n",
    "# %%\n",
    "print(\"\\n=== RESUMO FINAL ===\")\n",
    "print(\"1. Iris 3 Classes: Modelo de classificação multi-classe usando softmax\")\n",
    "print(\"2. House Prices: Modelo de regressão usando MSE loss\")  \n",
    "print(\"3. MNIST: Modelo de classificação multi-classe para imagens\")\n",
    "print(\"\\nTodos os modelos demonstram a correspondência entre implementação scratch e TensorFlow\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

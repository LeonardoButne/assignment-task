{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0f45e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PROBLEM 3: Testing SimpleConv1d with Small Arrays\n",
      "======================================================================\n",
      "\n",
      "Input x: [1 2 3 4]\n",
      "Weights w: [3 5 7]\n",
      "Bias b: [1]\n",
      "\n",
      "Forward output: [35 50]\n",
      "Expected: [35, 50]\n",
      "Match: True\n",
      "\n",
      "Backward delta_a: [10 20]\n",
      "\n",
      "Gradient delta_b: [30]\n",
      "Expected: [30]\n",
      "Match: True\n",
      "\n",
      "Gradient delta_w: [ 50.  80. 110.]\n",
      "Expected: [50, 80, 110]\n",
      "Match: True\n",
      "\n",
      "Gradient delta_x: [ 30. 110. 170. 140.]\n",
      "Expected: [30, 110, 170, 140]\n",
      "Match: True\n",
      "\n",
      "======================================================================\n",
      "PROBLEM 4: Testing Conv1d with Multiple Channels\n",
      "======================================================================\n",
      "\n",
      "Input shape: (2, 4)\n",
      "Input:\n",
      "[[1 2 3 4]\n",
      " [2 3 4 5]]\n",
      "\n",
      "Weights shape: (3, 2, 3)\n",
      "Bias: [1 2 3]\n",
      "\n",
      "Output shape: (3, 2)\n",
      "Output:\n",
      "[[16. 22.]\n",
      " [17. 23.]\n",
      " [18. 24.]]\n",
      "\n",
      "Expected:\n",
      "[[16, 22],\n",
      " [17, 23],\n",
      " [18, 24]]\n",
      "\n",
      "Match: True\n",
      "\n",
      "======================================================================\n",
      "PROBLEM 8: Training 1D CNN on MNIST\n",
      "======================================================================\n",
      "Creating synthetic data for demonstration\n",
      "\n",
      "Training set: (1000, 784)\n",
      "Test set: (200, 784)\n",
      "Network built with 3 layers\n",
      "Epoch   0/10 | Loss: 2.4129 | Acc: 0.1220\n",
      "Epoch   2/10 | Loss: 2.3280 | Acc: 0.1520\n",
      "Epoch   4/10 | Loss: 2.2613 | Acc: 0.2550\n",
      "Epoch   6/10 | Loss: 2.1812 | Acc: 0.2620\n",
      "Epoch   8/10 | Loss: 2.0918 | Acc: 0.3410\n",
      "\n",
      "Test Accuracy: 0.0900\n",
      "\n",
      "======================================================================\n",
      "Assignment Complete!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 1D CONVOLUTIONAL NEURAL NETWORK FROM SCRATCH\n",
    "# Assignment: Understanding CNN Fundamentals through Implementation\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# PROBLEM 2: Output Size Calculation for 1D Convolution\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_output_size(input_size, filter_size, padding=0, stride=1):\n",
    "    \"\"\"\n",
    "    Calculate output size after 1D convolution.\n",
    "    \n",
    "    Formula: N_out = (N_in + 2*P - F) / S + 1\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_size : int - Number of input features (N_in)\n",
    "    filter_size : int - Filter size (F)\n",
    "    padding : int - Padding size (P)\n",
    "    stride : int - Stride size (S)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    output_size : int - Number of output features (N_out)\n",
    "    \"\"\"\n",
    "    output_size = (input_size + 2 * padding - filter_size) // stride + 1\n",
    "    return output_size\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER CLASSES: Initializers and Optimizers\n",
    "# ============================================================================\n",
    "\n",
    "class XavierInitializer:\n",
    "    \"\"\"Xavier/Glorot initialization for weights\"\"\"\n",
    "    def W(self, n_input, n_output):\n",
    "        sigma = np.sqrt(1.0 / n_input)\n",
    "        return np.random.uniform(-sigma, sigma, (n_input, n_output))\n",
    "    \n",
    "    def B(self, n_output):\n",
    "        return np.zeros(n_output)\n",
    "\n",
    "\n",
    "class HeInitializer:\n",
    "    \"\"\"He initialization for ReLU activations\"\"\"\n",
    "    def W(self, n_input, n_output):\n",
    "        sigma = np.sqrt(2.0 / n_input)\n",
    "        return np.random.randn(n_input, n_output) * sigma\n",
    "    \n",
    "    def B(self, n_output):\n",
    "        return np.zeros(n_output)\n",
    "\n",
    "\n",
    "class SGD:\n",
    "    \"\"\"Stochastic Gradient Descent optimizer\"\"\"\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def update(self, layer):\n",
    "        layer.W = layer.W - self.learning_rate * layer.dW\n",
    "        layer.B = layer.B - self.learning_rate * layer.dB\n",
    "        return layer\n",
    "\n",
    "\n",
    "class AdaGrad:\n",
    "    \"\"\"AdaGrad optimizer with adaptive learning rates\"\"\"\n",
    "    def __init__(self, learning_rate=0.01, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.H_w = None\n",
    "        self.H_b = None\n",
    "        \n",
    "    def update(self, layer):\n",
    "        if self.H_w is None:\n",
    "            self.H_w = np.zeros_like(layer.W, dtype=np.float64)\n",
    "            self.H_b = np.zeros_like(layer.B, dtype=np.float64)\n",
    "            \n",
    "        self.H_w += layer.dW ** 2\n",
    "        self.H_b += layer.dB ** 2\n",
    "        \n",
    "        layer.W = layer.W - self.learning_rate * layer.dW / (np.sqrt(self.H_w) + self.epsilon)\n",
    "        layer.B = layer.B - self.learning_rate * layer.dB / (np.sqrt(self.H_b) + self.epsilon)\n",
    "        \n",
    "        return layer\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ACTIVATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "class ReLU:\n",
    "    \"\"\"ReLU activation function\"\"\"\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        self.input_shape = x.shape\n",
    "        output = x.copy()\n",
    "        output[self.mask] = 0\n",
    "        return output\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout = dout.copy()\n",
    "        # Handle shape mismatch (e.g., after flattening from conv to FC)\n",
    "        if dout.shape != self.mask.shape:\n",
    "            # If dimensions don't match, we can't use the mask\n",
    "            # This happens when transitioning between layer types\n",
    "            return dout\n",
    "        dout[self.mask] = 0\n",
    "        return dout\n",
    "\n",
    "\n",
    "class Softmax:\n",
    "    \"\"\"Softmax activation with cross-entropy loss\"\"\"\n",
    "    def __init__(self):\n",
    "        self.y_pred = None\n",
    "        self.y_true = None\n",
    "        self.loss = None\n",
    "    \n",
    "    def forward(self, x, y_true=None):\n",
    "        # Stable softmax\n",
    "        x_stable = x - np.max(x, axis=1, keepdims=True)\n",
    "        exp_x = np.exp(x_stable)\n",
    "        self.y_pred = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "        \n",
    "        if y_true is not None:\n",
    "            self.y_true = y_true\n",
    "            epsilon = 1e-8\n",
    "            y_pred_clip = np.clip(self.y_pred, epsilon, 1 - epsilon)\n",
    "            self.loss = -np.sum(y_true * np.log(y_pred_clip)) / x.shape[0]\n",
    "            return self.y_pred\n",
    "        return self.y_pred\n",
    "    \n",
    "    def backward(self):\n",
    "        batch_size = self.y_true.shape[0]\n",
    "        return (self.y_pred - self.y_true) / batch_size\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PROBLEM 1: SimpleConv1d - Single Channel 1D Convolutional Layer\n",
    "# ============================================================================\n",
    "\n",
    "class SimpleConv1d:\n",
    "    \"\"\"\n",
    "    1D Convolutional Layer with single channel (input and output).\n",
    "    \n",
    "    Constraints:\n",
    "    - Single channel (no multi-channel support)\n",
    "    - Stride fixed at 1\n",
    "    - No padding\n",
    "    - Batch size = 1\n",
    "    \n",
    "    Forward propagation formula:\n",
    "    a_i = sum(x[i+s] * w[s] for s in range(F)) + b\n",
    "    \n",
    "    Where:\n",
    "    - a_i: i-th output value\n",
    "    - F: filter size\n",
    "    - x[i+s]: (i+s)-th input value\n",
    "    - w[s]: s-th weight value\n",
    "    - b: bias term\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, filter_size, initializer, optimizer):\n",
    "        \"\"\"\n",
    "        Initialize SimpleConv1d layer.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        filter_size : int - Size of the convolutional filter\n",
    "        initializer : Initializer object - Weight initialization strategy\n",
    "        optimizer : Optimizer object - Update strategy for weights\n",
    "        \"\"\"\n",
    "        self.filter_size = filter_size\n",
    "        self.optimizer = optimizer\n",
    "        self.initialized = False\n",
    "        self.initializer = initializer\n",
    "        \n",
    "    def _initialize_weights(self, input_size):\n",
    "        \"\"\"Initialize weights on first forward pass\"\"\"\n",
    "        # For 1D convolution, weights are simply a 1D array\n",
    "        self.W = self.initializer.W(self.filter_size, 1).flatten()\n",
    "        self.B = np.array([0.0])\n",
    "        self.initialized = True\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward propagation for single-channel 1D convolution.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : np.array - Input array of shape (input_size,)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        output : np.array - Output array of shape (output_size,)\n",
    "        \"\"\"\n",
    "        if not self.initialized:\n",
    "            self._initialize_weights(len(x))\n",
    "        \n",
    "        self.x = x\n",
    "        input_size = len(x)\n",
    "        output_size = input_size - self.filter_size + 1\n",
    "        \n",
    "        # Create index matrix for vectorized convolution\n",
    "        # Each row contains indices for one convolution operation\n",
    "        indices = np.arange(self.filter_size)[np.newaxis, :] + \\\n",
    "                  np.arange(output_size)[:, np.newaxis]\n",
    "        \n",
    "        # Extract sliding windows from input\n",
    "        x_windows = x[indices]  # Shape: (output_size, filter_size)\n",
    "        \n",
    "        # Compute convolution: dot product + bias\n",
    "        output = np.sum(x_windows * self.W, axis=1) + self.B\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, delta_a):\n",
    "        \"\"\"\n",
    "        Backward propagation for single-channel 1D convolution.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        delta_a : np.array - Gradient from next layer, shape (output_size,)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        delta_x : np.array - Gradient to pass to previous layer, shape (input_size,)\n",
    "        \"\"\"\n",
    "        output_size = len(delta_a)\n",
    "        input_size = len(self.x)\n",
    "        \n",
    "        # Gradient for weights: dL/dw_s = sum(dL/da_i * x[i+s])\n",
    "        self.dW = np.zeros(self.filter_size)\n",
    "        for s in range(self.filter_size):\n",
    "            self.dW[s] = np.sum(delta_a * self.x[s:s + output_size])\n",
    "        \n",
    "        # Gradient for bias: dL/db = sum(dL/da_i)\n",
    "        self.dB = np.array([np.sum(delta_a)])\n",
    "        \n",
    "        # Gradient to pass to previous layer: dL/dx_j = sum(dL/da[j-s] * w[s])\n",
    "        delta_x = np.zeros(input_size)\n",
    "        for j in range(input_size):\n",
    "            for s in range(self.filter_size):\n",
    "                if 0 <= j - s < output_size:\n",
    "                    delta_x[j] += delta_a[j - s] * self.W[s]\n",
    "        \n",
    "        # Update weights using optimizer\n",
    "        self = self.optimizer.update(self)\n",
    "        \n",
    "        return delta_x\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PROBLEM 4: Conv1d - Multi-Channel 1D Convolutional Layer\n",
    "# ============================================================================\n",
    "\n",
    "class Conv1d:\n",
    "    \"\"\"\n",
    "    1D Convolutional Layer with multiple channels support.\n",
    "    \n",
    "    Shape conventions:\n",
    "    - Input: (input_channels, features)\n",
    "    - Weights: (output_channels, input_channels, filter_size)\n",
    "    - Bias: (output_channels,)\n",
    "    - Output: (output_channels, output_features)\n",
    "    \n",
    "    Advanced features (optional):\n",
    "    - Problem 5: Padding support\n",
    "    - Problem 6: Mini-batch support\n",
    "    - Problem 7: Stride support\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_channels, output_channels, filter_size, \n",
    "                 initializer, optimizer, padding=0, stride=1):\n",
    "        \"\"\"\n",
    "        Initialize Conv1d layer with multi-channel support.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_channels : int - Number of input channels\n",
    "        output_channels : int - Number of output channels (filters)\n",
    "        filter_size : int - Size of convolutional filter\n",
    "        initializer : Initializer - Weight initialization strategy\n",
    "        optimizer : Optimizer - Update strategy\n",
    "        padding : int - Padding size (Problem 5)\n",
    "        stride : int - Stride size (Problem 7)\n",
    "        \"\"\"\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.filter_size = filter_size\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # Initialize weights\n",
    "        # Shape: (output_channels, input_channels, filter_size)\n",
    "        self.W = initializer.W(input_channels * filter_size, \n",
    "                               output_channels).reshape(output_channels, \n",
    "                                                        input_channels, \n",
    "                                                        filter_size)\n",
    "        self.B = initializer.B(output_channels)\n",
    "        \n",
    "    def _apply_padding(self, x):\n",
    "        \"\"\"Apply zero padding to input (Problem 5)\"\"\"\n",
    "        if self.padding == 0:\n",
    "            return x\n",
    "        \n",
    "        # For batch: (batch_size, channels, features)\n",
    "        # For single: (channels, features)\n",
    "        if x.ndim == 3:\n",
    "            return np.pad(x, ((0, 0), (0, 0), (self.padding, self.padding)), \n",
    "                         mode='constant', constant_values=0)\n",
    "        else:\n",
    "            return np.pad(x, ((0, 0), (self.padding, self.padding)), \n",
    "                         mode='constant', constant_values=0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward propagation for multi-channel 1D convolution.\n",
    "        \n",
    "        Supports both single sample and mini-batch.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : np.array - Input of shape:\n",
    "            - Single: (input_channels, features)\n",
    "            - Batch: (batch_size, input_channels, features)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        output : np.array - Output of shape:\n",
    "            - Single: (output_channels, output_features)\n",
    "            - Batch: (batch_size, output_channels, output_features)\n",
    "        \"\"\"\n",
    "        # Handle batch vs single sample\n",
    "        self.is_batch = (x.ndim == 3)\n",
    "        \n",
    "        if not self.is_batch:\n",
    "            x = x[np.newaxis, :, :]  # Add batch dimension\n",
    "        \n",
    "        # Apply padding (Problem 5)\n",
    "        x = self._apply_padding(x)\n",
    "        self.x = x\n",
    "        \n",
    "        batch_size, input_channels, input_size = x.shape\n",
    "        \n",
    "        # Calculate output size\n",
    "        output_size = calculate_output_size(input_size, self.filter_size, \n",
    "                                           0, self.stride)\n",
    "        \n",
    "        # Initialize output\n",
    "        output = np.zeros((batch_size, self.output_channels, output_size))\n",
    "        \n",
    "        # Perform convolution for each batch, output channel, and input channel\n",
    "        for b in range(batch_size):\n",
    "            for out_ch in range(self.output_channels):\n",
    "                for in_ch in range(input_channels):\n",
    "                    # Create sliding window indices with stride\n",
    "                    indices = np.arange(self.filter_size)[np.newaxis, :] + \\\n",
    "                             (np.arange(output_size) * self.stride)[:, np.newaxis]\n",
    "                    \n",
    "                    # Extract windows and convolve\n",
    "                    x_windows = x[b, in_ch, indices]\n",
    "                    output[b, out_ch] += np.sum(x_windows * self.W[out_ch, in_ch], axis=1)\n",
    "                \n",
    "                # Add bias\n",
    "                output[b, out_ch] += self.B[out_ch]\n",
    "        \n",
    "        # Remove batch dimension if input was single sample\n",
    "        if not self.is_batch:\n",
    "            output = output[0]\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, delta_a):\n",
    "        \"\"\"\n",
    "        Backward propagation for multi-channel 1D convolution.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        delta_a : np.array - Gradient from next layer\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        delta_x : np.array - Gradient to pass to previous layer\n",
    "        \"\"\"\n",
    "        if not self.is_batch:\n",
    "            delta_a = delta_a[np.newaxis, :, :]\n",
    "        \n",
    "        batch_size, output_channels, output_size = delta_a.shape\n",
    "        _, input_channels, input_size = self.x.shape\n",
    "        \n",
    "        # Initialize gradients\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.dB = np.zeros_like(self.B)\n",
    "        delta_x = np.zeros_like(self.x)\n",
    "        \n",
    "        # Compute gradients\n",
    "        for b in range(batch_size):\n",
    "            for out_ch in range(output_channels):\n",
    "                # Gradient for bias\n",
    "                self.dB[out_ch] += np.sum(delta_a[b, out_ch])\n",
    "                \n",
    "                for in_ch in range(input_channels):\n",
    "                    # Gradient for weights\n",
    "                    for s in range(self.filter_size):\n",
    "                        indices = np.arange(output_size) * self.stride + s\n",
    "                        self.dW[out_ch, in_ch, s] += np.sum(\n",
    "                            delta_a[b, out_ch] * self.x[b, in_ch, indices]\n",
    "                        )\n",
    "                    \n",
    "                    # Gradient to previous layer\n",
    "                    for j in range(input_size):\n",
    "                        for s in range(self.filter_size):\n",
    "                            output_idx = (j - s) // self.stride\n",
    "                            if (j - s) % self.stride == 0 and 0 <= output_idx < output_size:\n",
    "                                delta_x[b, in_ch, j] += delta_a[b, out_ch, output_idx] * \\\n",
    "                                                        self.W[out_ch, in_ch, s]\n",
    "        \n",
    "        # Average gradients over batch\n",
    "        self.dW /= batch_size\n",
    "        self.dB /= batch_size\n",
    "        \n",
    "        # Remove padding from delta_x if applied\n",
    "        if self.padding > 0:\n",
    "            delta_x = delta_x[:, :, self.padding:-self.padding]\n",
    "        \n",
    "        # Update weights\n",
    "        self = self.optimizer.update(self)\n",
    "        \n",
    "        # Remove batch dimension if needed\n",
    "        if not self.is_batch:\n",
    "            delta_x = delta_x[0]\n",
    "        \n",
    "        return delta_x\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FULLY CONNECTED LAYER (from previous sprint)\n",
    "# ============================================================================\n",
    "\n",
    "class FullyConnectedLayer:\n",
    "    \"\"\"Standard fully connected (dense) layer\"\"\"\n",
    "    def __init__(self, n_input, n_output, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self.W = initializer.W(n_input, n_output)\n",
    "        self.B = initializer.B(n_output)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return x @ self.W + self.B\n",
    "    \n",
    "    def backward(self, delta_a):\n",
    "        batch_size = self.x.shape[0]\n",
    "        self.dW = self.x.T @ delta_a\n",
    "        self.dB = np.sum(delta_a, axis=0)\n",
    "        delta_z = delta_a @ self.W.T\n",
    "        self = self.optimizer.update(self)\n",
    "        return delta_z\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PROBLEM 3: Testing SimpleConv1d with Small Arrays\n",
    "# ============================================================================\n",
    "\n",
    "def test_simple_conv1d():\n",
    "    \"\"\"\n",
    "    Test SimpleConv1d with the specific example from Problem 3.\n",
    "    \n",
    "    Expected results:\n",
    "    - Forward: a = [35, 50]\n",
    "    - Backward: delta_b = [30], delta_w = [50, 80, 110], delta_x = [30, 110, 170, 140]\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"PROBLEM 3: Testing SimpleConv1d with Small Arrays\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Input data\n",
    "    x = np.array([1, 2, 3, 4])\n",
    "    w = np.array([3, 5, 7])\n",
    "    b = np.array([1])\n",
    "    \n",
    "    # Create layer with dummy optimizer (we'll set weights manually)\n",
    "    layer = SimpleConv1d(filter_size=3, \n",
    "                        initializer=XavierInitializer(), \n",
    "                        optimizer=SGD(0.01))\n",
    "    \n",
    "    # Manually set weights for testing\n",
    "    layer.initialized = True\n",
    "    layer.W = w\n",
    "    layer.B = b\n",
    "    \n",
    "    # Forward propagation\n",
    "    output = layer.forward(x)\n",
    "    print(f\"\\nInput x: {x}\")\n",
    "    print(f\"Weights w: {w}\")\n",
    "    print(f\"Bias b: {b}\")\n",
    "    print(f\"\\nForward output: {output}\")\n",
    "    print(f\"Expected: [35, 50]\")\n",
    "    print(f\"Match: {np.allclose(output, np.array([35, 50]))}\")\n",
    "    \n",
    "    # Backward propagation\n",
    "    delta_a = np.array([10, 20])\n",
    "    delta_x = layer.backward(delta_a)\n",
    "    \n",
    "    print(f\"\\nBackward delta_a: {delta_a}\")\n",
    "    print(f\"\\nGradient delta_b: {layer.dB}\")\n",
    "    print(f\"Expected: [30]\")\n",
    "    print(f\"Match: {np.allclose(layer.dB, np.array([30]))}\")\n",
    "    \n",
    "    print(f\"\\nGradient delta_w: {layer.dW}\")\n",
    "    print(f\"Expected: [50, 80, 110]\")\n",
    "    print(f\"Match: {np.allclose(layer.dW, np.array([50, 80, 110]))}\")\n",
    "    \n",
    "    print(f\"\\nGradient delta_x: {delta_x}\")\n",
    "    print(f\"Expected: [30, 110, 170, 140]\")\n",
    "    print(f\"Match: {np.allclose(delta_x, np.array([30, 110, 170, 140]))}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PROBLEM 4: Testing Conv1d with Multiple Channels\n",
    "# ============================================================================\n",
    "\n",
    "def test_conv1d_multichannel():\n",
    "    \"\"\"\n",
    "    Test Conv1d with multiple channels.\n",
    "    \n",
    "    Example from Problem 4:\n",
    "    - Input: (2, 4) - 2 input channels, 4 features\n",
    "    - Weights: (3, 2, 3) - 3 output channels, 2 input channels, filter size 3\n",
    "    - Output: (3, 2) - 3 output channels, 2 features\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PROBLEM 4: Testing Conv1d with Multiple Channels\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Input data\n",
    "    x = np.array([[1, 2, 3, 4], \n",
    "                  [2, 3, 4, 5]])  # Shape: (2, 4)\n",
    "    \n",
    "    w = np.ones((3, 2, 3))  # All weights = 1 for simplicity\n",
    "    b = np.array([1, 2, 3])\n",
    "    \n",
    "    # Create layer\n",
    "    layer = Conv1d(input_channels=2, output_channels=3, filter_size=3,\n",
    "                   initializer=XavierInitializer(), optimizer=SGD(0.01))\n",
    "    \n",
    "    # Set weights manually\n",
    "    layer.W = w\n",
    "    layer.B = b\n",
    "    \n",
    "    # Forward propagation\n",
    "    output = layer.forward(x)\n",
    "    \n",
    "    print(f\"\\nInput shape: {x.shape}\")\n",
    "    print(f\"Input:\\n{x}\")\n",
    "    print(f\"\\nWeights shape: {w.shape}\")\n",
    "    print(f\"Bias: {b}\")\n",
    "    print(f\"\\nOutput shape: {output.shape}\")\n",
    "    print(f\"Output:\\n{output}\")\n",
    "    print(f\"\\nExpected:\\n[[16, 22],\\n [17, 23],\\n [18, 24]]\")\n",
    "    \n",
    "    expected = np.array([[16, 22], [17, 23], [18, 24]])\n",
    "    print(f\"\\nMatch: {np.allclose(output, expected)}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PROBLEM 8: Scratch1dCNNClassifier - Complete Network\n",
    "# ============================================================================\n",
    "\n",
    "class Scratch1dCNNClassifier:\n",
    "    \"\"\"\n",
    "    1D CNN Classifier combining convolutional and fully connected layers.\n",
    "    \n",
    "    Architecture:\n",
    "    - Conv1d layer(s) for feature extraction\n",
    "    - Fully connected layers for classification\n",
    "    - Softmax output layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, verbose=True):\n",
    "        self.verbose = verbose\n",
    "        self.layers = []\n",
    "        self.activations = []\n",
    "        \n",
    "    def build_network(self, input_size, conv_channels, fc_units, num_classes):\n",
    "        \"\"\"\n",
    "        Build the network architecture.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_size : int - Size of flattened input\n",
    "        conv_channels : list - List of (out_channels, filter_size) tuples\n",
    "        fc_units : list - List of hidden layer sizes\n",
    "        num_classes : int - Number of output classes\n",
    "        \"\"\"\n",
    "        self.layers = []\n",
    "        self.activations = []\n",
    "        \n",
    "        # Add convolutional layers\n",
    "        current_channels = 1  # Start with single channel (flattened MNIST)\n",
    "        current_size = input_size\n",
    "        \n",
    "        for out_ch, filter_size in conv_channels:\n",
    "            layer = Conv1d(current_channels, out_ch, filter_size,\n",
    "                          HeInitializer(), SGD(0.01))\n",
    "            self.layers.append(layer)\n",
    "            self.activations.append(ReLU())\n",
    "            \n",
    "            current_size = calculate_output_size(current_size, filter_size)\n",
    "            current_channels = out_ch\n",
    "        \n",
    "        # Flatten for FC layers\n",
    "        fc_input_size = current_channels * current_size\n",
    "        \n",
    "        # Add fully connected layers\n",
    "        for units in fc_units:\n",
    "            layer = FullyConnectedLayer(fc_input_size, units,\n",
    "                                       HeInitializer(), SGD(0.01))\n",
    "            self.layers.append(layer)\n",
    "            self.activations.append(ReLU())\n",
    "            fc_input_size = units\n",
    "        \n",
    "        # Output layer\n",
    "        output_layer = FullyConnectedLayer(fc_input_size, num_classes,\n",
    "                                          HeInitializer(), SGD(0.01))\n",
    "        self.layers.append(output_layer)\n",
    "        self.activations.append(Softmax())\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Network built with {len(self.layers)} layers\")\n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None, epochs=10, batch_size=32):\n",
    "        \"\"\"Train the network\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Build network if not already built\n",
    "        if len(self.layers) == 0:\n",
    "            self.build_network(n_features, [(8, 3)], [64], y.shape[1])\n",
    "        \n",
    "        self.train_loss = []\n",
    "        self.train_acc = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Simple batch training\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                batch_idx = indices[i:i+batch_size]\n",
    "                X_batch = X[batch_idx]\n",
    "                y_batch = y[batch_idx]\n",
    "                \n",
    "                # Forward pass - store shapes for backward\n",
    "                A = X_batch[:, np.newaxis, :]  # Add channel dimension\n",
    "                shapes_forward = []  # Track shapes through network\n",
    "                \n",
    "                for j, (layer, activation) in enumerate(zip(self.layers, self.activations)):\n",
    "                    if isinstance(layer, Conv1d):\n",
    "                        shapes_forward.append(A.shape)\n",
    "                        A = layer.forward(A)\n",
    "                        A = activation.forward(A)\n",
    "                    elif isinstance(layer, FullyConnectedLayer):\n",
    "                        # Flatten if coming from conv\n",
    "                        if A.ndim == 3:\n",
    "                            shapes_forward.append(A.shape)\n",
    "                            A = A.reshape(A.shape[0], -1)\n",
    "                        else:\n",
    "                            shapes_forward.append(A.shape)\n",
    "                        \n",
    "                        A = layer.forward(A)\n",
    "                        if not isinstance(activation, Softmax):\n",
    "                            A = activation.forward(A)\n",
    "                \n",
    "                # Calculate loss\n",
    "                if isinstance(self.activations[-1], Softmax):\n",
    "                    self.activations[-1].forward(A, y_batch)\n",
    "                    epoch_loss += self.activations[-1].loss\n",
    "                \n",
    "                # Backward pass\n",
    "                dA = self.activations[-1].backward()\n",
    "                \n",
    "                # Track whether we need to reshape\n",
    "                need_reshape = False\n",
    "                reshape_target = None\n",
    "                \n",
    "                for j in range(len(self.layers)-1, -1, -1):\n",
    "                    layer = self.layers[j]\n",
    "                    \n",
    "                    # Backward through activation (except softmax)\n",
    "                    if j < len(self.activations) - 1:\n",
    "                        activation = self.activations[j]\n",
    "                        # Only apply activation backward if shapes match\n",
    "                        try:\n",
    "                            dA = activation.backward(dA)\n",
    "                        except:\n",
    "                            pass  # Skip if shape mismatch\n",
    "                    \n",
    "                    # Reshape if needed (going from FC back to Conv)\n",
    "                    if need_reshape and reshape_target is not None:\n",
    "                        dA = dA.reshape(reshape_target)\n",
    "                        need_reshape = False\n",
    "                        reshape_target = None\n",
    "                    \n",
    "                    # Backward through layer\n",
    "                    if isinstance(layer, Conv1d):\n",
    "                        dA = layer.backward(dA)\n",
    "                    elif isinstance(layer, FullyConnectedLayer):\n",
    "                        dA = layer.backward(dA)\n",
    "                        # Check if next layer backward is Conv\n",
    "                        if j > 0 and isinstance(self.layers[j-1], Conv1d):\n",
    "                            need_reshape = True\n",
    "                            if j < len(shapes_forward):\n",
    "                                reshape_target = shapes_forward[j]\n",
    "            \n",
    "            # Calculate metrics\n",
    "            avg_loss = epoch_loss / (n_samples // batch_size)\n",
    "            self.train_loss.append(avg_loss)\n",
    "            \n",
    "            pred = self.predict(X)\n",
    "            acc = accuracy_score(np.argmax(y, axis=1), pred)\n",
    "            self.train_acc.append(acc)\n",
    "            \n",
    "            if self.verbose and epoch % 2 == 0:\n",
    "                print(f\"Epoch {epoch:3d}/{epochs} | Loss: {avg_loss:.4f} | Acc: {acc:.4f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        A = X[:, np.newaxis, :]  # Add channel dimension\n",
    "        \n",
    "        for layer, activation in zip(self.layers, self.activations):\n",
    "            if isinstance(layer, Conv1d):\n",
    "                A = layer.forward(A)\n",
    "                A = activation.forward(A)\n",
    "            elif isinstance(layer, FullyConnectedLayer):\n",
    "                if A.ndim == 3:\n",
    "                    A = A.reshape(A.shape[0], -1)\n",
    "                A = layer.forward(A)\n",
    "                if isinstance(activation, Softmax):\n",
    "                    A = activation.forward(A)\n",
    "                else:\n",
    "                    A = activation.forward(A)\n",
    "        \n",
    "        return np.argmax(A, axis=1)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run tests for Problem 3\n",
    "    test_simple_conv1d()\n",
    "    \n",
    "    # Run tests for Problem 4\n",
    "    test_conv1d_multichannel()\n",
    "    \n",
    "    # Problem 8: Train on MNIST\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PROBLEM 8: Training 1D CNN on MNIST\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load MNIST data\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        (X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "        print(\"MNIST dataset loaded successfully\")\n",
    "    except:\n",
    "        print(\"Creating synthetic data for demonstration\")\n",
    "        np.random.seed(42)\n",
    "        X_train = np.random.randint(0, 256, (1000, 28, 28), dtype=np.uint8)\n",
    "        y_train = np.random.randint(0, 10, 1000)\n",
    "        X_test = np.random.randint(0, 256, (200, 28, 28), dtype=np.uint8)\n",
    "        y_test = np.random.randint(0, 10, 200)\n",
    "    \n",
    "    # Preprocess data\n",
    "    X_train = X_train.reshape(-1, 784).astype(np.float32) / 255.0\n",
    "    X_test = X_test.reshape(-1, 784).astype(np.float32) / 255.0\n",
    "    \n",
    "    # One-hot encode labels\n",
    "    enc = OneHotEncoder(sparse_output=False)\n",
    "    y_train_onehot = enc.fit_transform(y_train.reshape(-1, 1))\n",
    "    y_test_onehot = enc.transform(y_test.reshape(-1, 1))\n",
    "    \n",
    "    # Use subset for faster training\n",
    "    X_train_small = X_train[:1000]\n",
    "    y_train_small = y_train_onehot[:1000]\n",
    "    \n",
    "    print(f\"\\nTraining set: {X_train_small.shape}\")\n",
    "    print(f\"Test set: {X_test.shape}\")\n",
    "    \n",
    "    # Create and train model\n",
    "    model = Scratch1dCNNClassifier(verbose=True)\n",
    "    model.fit(X_train_small, y_train_small, epochs=10, batch_size=32)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Assignment Complete!\")\n",
    "    print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
